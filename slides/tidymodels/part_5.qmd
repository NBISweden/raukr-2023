---
title: "tidymodels - Feature engineering"
author: "Max Kuhn"
image: "images/featured.png"
format:
  revealjs:
    slide-number: true
    code-line-numbers: true
    footer: <https://nbisweden.github.io/raukr-2023>
    include-before-body: styles/header.html
    include-after-body: styles/footer-annotations.html
    theme: [default, styles/tidymodels.scss]
    width: 1280
    height: 720
knitr:
  opts_chunk:
    echo: true
    collapse: true
    comment: "#>"
    fig.align: "center"

fig-format: svg
---

## Previously...

```{r}
#| label: startup

library(tidymodels)
library(doParallel)

tidymodels_prefer()
theme_set(theme_bw())
options(pillar.advice = FALSE, pillar.min_title_chars = Inf)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)

data(cells, package = "modeldata")
cells$case <- NULL

set.seed(123)
cell_split <- initial_split(cells, prop = 0.8, strata = class)
cell_tr <- training(cell_split)
cell_te <- testing(cell_split)

set.seed(123)
cell_rs <- vfold_cv(cell_tr, v = 10, strata = class)

cls_metrics <- metric_set(brier_class, roc_auc, kap)
```


## Working with our predictors

We might want to modify our predictors columns for a few reasons: 

::: {.incremental}
- The model requires them in a different format (e.g. dummy variables for `lm()`).
- The model needs certain data qualities (e.g. same units for K-NN).
- The outcome is better predicted when one or more columns are transformed in some way (a.k.a "feature engineering"). 
:::

. . .

The first two reasons are fairly predictable ([next page](https://www.tmwr.org/pre-proc-table.html#tab:preprocessing)).

The last one depends on your modeling problem. 


##  {background-iframe="https://www.tmwr.org/pre-proc-table.html#tab:preprocessing"}

::: footer
:::


## What is feature engineering?

Think of a feature as some *representation* of a predictor that will be used in a model.

. . .

Example representations:

-   Interactions
-   Polynomial expansions/splines
-   PCA feature extraction

There are a lot of examples in [_Feature Engineering and Selection_](https://bookdown.org/max/FES/).



## Example: Dates

How can we represent date columns for our model?

. . .

When a date column is used in its native format, it is usually converted by an R model to an integer.

. . .

It can be re-engineered as:

-   Days since a reference date
-   Day of the week
-   Month
-   Year
-   Indicators for holidays

::: notes
The main point is that we try to maximize performance with different versions of the predictors. 

Mention that, for the Chicago data, the day or the week features are usually the most important ones in the model.
:::

## General definitions 

-   *Data preprocessing* steps allow your model to fit.

-   *Feature engineering* steps help the model do the least work to predict the outcome as well as possible.

The recipes package can handle both!

In a little bit, we'll see successful (and unsuccessful) feature engineering methods for our example data. 


::: notes
These terms are often used interchangeably in the ML community but we want to distinguish them.
:::


## Prepare your data for modeling

- The recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.

. . .

- Statistical parameters for the steps can be _estimated_ from an initial data set and then _applied_ to other data sets.

. . .

- The resulting processed output can be used as inputs for statistical or machine learning models.

## A first recipe 

```{r base-recipe}
cell_rec <- 
  recipe(class ~ ., data = cell_tr) 
```

. . .

- The `recipe()` function assigns columns to roles of "outcome" or "predictor" using the formula

## A first recipe 

```{r rec-summary}
summary(cell_rec)
```

## Transforming individual predictors

```{r}
#| code-line-numbers: "3"
cell_rec <- 
  recipe(class ~ ., data = cell_tr) %>% 
  step_YeoJohnson(all_predictors())
```

. . .

The YJ transformation can be used to produce more symmetric distirbutions for predictors. It is very similar to the Box-Cox transformation. 


## Standardize predictors

```{r}
#| code-line-numbers: "4"
pca_rec <- 
  recipe(class ~ ., data = cell_tr) %>% 
  step_YeoJohnson(all_predictors()) %>% 
  step_normalize(all_predictors())
```

. . .

- This centers and scales the numeric predictors.


- The recipe will use the _training_ set to estimate the means and standard deviations of the data.

. . .

- All data the recipe is applied to will be normalized using those statistics (there is no re-estimation).

## Convert the data to PCA components

```{r}
#| code-line-numbers: "5"
pca_rec <- 
  recipe(class ~ ., data = cell_tr) %>% 
  step_YeoJohnson(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = 10)
```


## Convert the data to PLS components

```{r rec-norm}
#| eval: false
#| code-line-numbers: "5"
pca_rec <- 
  recipe(class ~ ., data = cell_tr) %>% 
  step_YeoJohnson(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_pls(all_predictors(), outcome = vars(class), num_comp = 10)
```

. . .

Since PLS is supervised, we have to use the `outcome` argument. 


## Reduce correlation 

```{r }
#| code-line-numbers: "4"
#| eval: false
filter_rec <- 
  recipe(class ~ ., data = cell_tr) %>% 
  step_YeoJohnson(all_predictors()) %>% 
  step_corr(all_numeric_predictors(), threshold = 0.9)
```

. . .

To deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold.



## Using a workflow

```{r}
#| cache: true
cell_pca_wflow <-
  workflow() %>%
  add_recipe(pca_rec) %>%
  add_model(logistic_reg())
 
ctrl <- control_resamples(save_pred = TRUE)

set.seed(9)
cell_glm_res <-
  cell_pca_wflow %>%
  fit_resamples(cell_rs, control = ctrl, metrics = cls_metrics)

collect_metrics(cell_glm_res)
```


## Recipes are estimated 

Preprocessing steps in a recipe use the *training set* to compute quantities.

. . .

What kind of quantities are computed for preprocessing?

-   Levels of a factor
-   Whether a column has zero variance
-   Normalization
-   Feature extraction
-   Effect encodings

. . .

When a recipe is part of a workflow, this estimation occurs when `fit()` is called.

_The recipe is estimated within each resample_.

## Getting specific results

:::: {.columns}

::: {.column width="50%"}

```{r resample-encoding}
cell_pca_fit <-
  cell_pca_wflow %>% 
  fit(data = cell_tr)

cell_pca_fit %>% 
  extract_recipe() %>% 
  tidy(number = 1)
```

:::

::: {.column width="50%"}

```{r}
#| label: tidy-model
cell_pca_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```
:::

::::





## Debugging a recipe

- Typically, you will want to use a workflow to estimate and apply a recipe.

. . .

- If you have an error and need to debug your recipe, the original recipe object (e.g. `pca_rec`) can be estimated manually with a function called `prep()`. It is analogous to `fit()`. See [TMwR section 16.4](https://www.tmwr.org/dimensionality.html#recipe-functions)

. . .

- Another function (`bake()`) is analogous to `predict()`, and gives you the processed data back.

. . .

- The `tidy()` function can be used to get specific results from the recipe.

## More on recipes

-   Once `fit()` is called on a workflow, changing the model does not re-fit the recipe.

. . .

-   A list of all known steps is at <https://www.tidymodels.org/find/recipes/>.

. . .

-   Some steps can be [skipped](https://recipes.tidymodels.org/articles/Skipping.html) when using `predict()`.

. . .

-   The [order](https://recipes.tidymodels.org/articles/Ordering.html) of the steps matters.

```{r teardown}
#| include: false

parallel::stopCluster(cl)
```
