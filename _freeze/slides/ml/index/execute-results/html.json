{
  "hash": "d3817cc3cf55e92e271ab47def0bdb97",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Machine Learning\"\nauthor: \"Nikolay Oskolkov\"\nimage: \"assets/featured.jpg\"\nformat:\n  revealjs:\n    fig-align: center\n---\n\n\n## Packages {visibility=\"hidden\"}\n\n\n::: {.cell}\n\n:::\n\n\n## Different Types of Data Analysis\n\n- Depends on the amount of data we have\n- Balance between the numbers of features and observations\n  - P is the number of features (genes, proteins, genetic variants etc.)\n  - N is the number of observations (samples, cells, nucleotides etc.)\n\n![](assets/AmountOfData.jpg){width=\"90%\"}\n\n## The Curse of Dimensionality\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n![](assets/DarkMagic.jpg)\n\n:::\n::: {.column width=\"60%\"}\n\n**This is how data looks like in high dimensions:**\n\n![](assets/ndim_ball.jpg)\n\n$$Y = \\alpha + \\beta X$$\n$$\\beta = \\left(X^TX\\right)^{-1}X^TY$$\n\n$$\\left(X^TX\\right)^{-1} \\sim \\frac{1}{\\rm{det}\\left(X^TX\\right)}\\dots\\,\\rightarrow\\,\\infty\\hbox{,}\\,\\,\\,\\,\\,\\,\\,\\,n\\ll p$$\n\n**Inverted covariance matrix diverges in high dimensions**\n\n<br/>\n\n$$E[\\hat{\\sigma}^2]=\\frac{n-p}{n}\\sigma^2$$\n\n**Maximum Likelihood variance estimator is severely biased in high dimensions**\n\n:::\n::::\n\n## Equidistant Points in High Dimensions\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n\n\n::: {.cell hash='index_cache/revealjs/unnamed-chunk-2_71d827a35842e0b5ef5f2f4675d74f02'}\n\n```{.r .cell-code}\nn <- 1000;\npair_dist <- list();\np <- c(2, 32, 512)\n\nfor(i in 1:length(p)) {\n  X <- matrix(rnorm(n * p[i]), n, p[i])\n  pair_dist[[i]] <- as.vector(dist(X));\n  pair_dist[[i]] <- pair_dist[[i]] / max(pair_dist[[i]])\n}\n```\n:::\n\n\n:::\n::: {.column width=\"30%\"}\n\n\n::: {.cell hash='index_cache/revealjs/Hist_Plot_2c457d68866eecbe924c7ecffec79d21'}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/Hist_Plot-1.png){width=384}\n:::\n:::\n\n\n:::\n::::\n\n## Neighboring Points in High Dimensions\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n\n\n::: {.cell hash='index_cache/revealjs/unnamed-chunk-3_da654db0eaf5179ae1c457e313766ed1'}\n\n```{.r .cell-code}\nn <- 1000\nfirst_dist <- vector()\nmid_dist <- vector()\nlast_dist <- vector()\nnum_dim <- 2 ^ seq(1, 10, by=1)\nfor(p in num_dim) {\n  X <- matrix(rnorm(n*p), n, p)\n  dist_X <- sort(as.vector(dist(X)), decreasing=FALSE)\n  first_dist <- append(first_dist, dist_X[1])\n  mid_dist <- append(mid_dist, dist_X[round(length(dist_X)/2, 0)])\n  last_dist <- append(last_dist, dist_X[length(dist_X)])\n}\n```\n:::\n\n\n:::\n::: {.column width=\"30%\"}\n\n\n::: {.cell hash='index_cache/revealjs/Neighbors Plot_26274b55c89e7b5fe96df43963c190f8'}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/Neighbors Plot-1.png){width=384}\n:::\n:::\n\n\n:::\n::::\n\n## Low Dimensional Space\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 20 # number of samples\np <- 2  # number of features / dimensions\nY <- rnorm(n)\nX <- matrix(rnorm(n*p),n,p)\nsummary(lm(Y~X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0522 -0.6380  0.1451  0.3911  1.8829 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.14950    0.22949   0.651    0.523\nX1          -0.09405    0.28245  -0.333    0.743\nX2          -0.11919    0.24486  -0.487    0.633\n\nResidual standard error: 1.017 on 17 degrees of freedom\nMultiple R-squared:  0.02204,\tAdjusted R-squared:  -0.09301 \nF-statistic: 0.1916 on 2 and 17 DF,  p-value: 0.8274\n```\n:::\n:::\n\n\n## Going to Higher Dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456); n <- 20; p <- 10\nY <- rnorm(n); X <- matrix(rnorm(n*p),n,p); summary(lm(Y~X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0255 -0.4320  0.1056  0.4493  1.0617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.54916    0.26472   2.075   0.0679 .\nX1           0.30013    0.21690   1.384   0.1998  \nX2           0.68053    0.27693   2.457   0.0363 *\nX3          -0.10675    0.26010  -0.410   0.6911  \nX4          -0.21367    0.33690  -0.634   0.5417  \nX5          -0.19123    0.31881  -0.600   0.5634  \nX6           0.81074    0.25221   3.214   0.0106 *\nX7           0.09634    0.24143   0.399   0.6992  \nX8          -0.29864    0.19004  -1.571   0.1505  \nX9          -0.78175    0.35408  -2.208   0.0546 .\nX10          0.83736    0.36936   2.267   0.0496 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8692 on 9 degrees of freedom\nMultiple R-squared:  0.6592,\tAdjusted R-squared:  0.2805 \nF-statistic: 1.741 on 10 and 9 DF,  p-value: 0.2089\n```\n:::\n:::\n\n\n## Even Higher Dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456); n <- 20; p <- 20\nY <- rnorm(n); X <- matrix(rnorm(n*p),n,p); summary(lm(Y~X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\nALL 20 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  1.34889        NaN     NaN      NaN\nX1           0.66218        NaN     NaN      NaN\nX2           0.76212        NaN     NaN      NaN\nX3          -1.35033        NaN     NaN      NaN\nX4          -0.57487        NaN     NaN      NaN\nX5           0.02142        NaN     NaN      NaN\nX6           0.40290        NaN     NaN      NaN\nX7           0.03313        NaN     NaN      NaN\nX8          -0.31983        NaN     NaN      NaN\nX9          -0.92833        NaN     NaN      NaN\nX10          0.18091        NaN     NaN      NaN\nX11         -1.37618        NaN     NaN      NaN\nX12          2.11438        NaN     NaN      NaN\nX13         -1.75103        NaN     NaN      NaN\nX14         -1.55073        NaN     NaN      NaN\nX15          0.01112        NaN     NaN      NaN\nX16         -0.50943        NaN     NaN      NaN\nX17         -0.47576        NaN     NaN      NaN\nX18          0.31793        NaN     NaN      NaN\nX19          1.43615        NaN     NaN      NaN\nX20               NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:    NaN \nF-statistic:   NaN on 19 and 0 DF,  p-value: NA\n```\n:::\n:::\n\n\n## High dimensional data in Life Sciences\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n- Proteomics, metabolomics: 50 - 100\n- Metagenomics: 100 - 500\n- Transcriptomics: 20 000\n- Microscopy imaging: 1 000 000\n- Genomics, epigenomics: 30 000 000\n\n:::\n::: {.column width=\"60%\"}\n\n![](assets/DataLifeSciences.jpg)\n\n:::\n::::\n\n## Ways to Overcome The Curse of Dimensionality\n\n* Increasing sample size - often cannot afford\n* Dimensionality reduction - PCA, UMAP etc.\n* Regularizations - LASSO, Ridge, Elastic Net, Dropout etc.\n* Bayesian statistics - Prior plays a role of Regularization\n\n![](assets/Overcome_CD.jpg)\n\n## Dimensionality Reduction\n\n![](assets/PCA_tSNE_MNIST.jpg){width=\"90%\"}\n\n## Dimension Reduction: Matrix Factorization\n\n![](assets/MatrixFactor.jpg)\n\n## Dimension Reduction: Neighborhood Graph\n\n![](assets/tSNE_Scheme.jpg){width=\"68%\"}\n\n## Regularizations: LASSO\n\n$$Y = \\beta_1X_1+\\beta_2X_2+\\epsilon \\\\\n\\textrm{OLS} = (Y-\\beta_1X_1-\\beta_2X_2)^2 \\\\\n\\textrm{Penalized OLS} = (Y-\\beta_1X_1-\\beta_2X_2)^2 + \\lambda(|\\beta_1|+|\\beta_2|)$$\n\n![](assets/CV_lambda.jpg){width=\"70%\"}\n\n## Regularizations: K-Fold Cross-Validation\n\n![](assets/Kfold_CrossVal.jpg)\n\n## Priors in Bayesian Statistics - Regularizations\n\n$$Y = \\beta_1X_1+\\beta_2X_2+\\epsilon$$\n* **Maximum Likelihood** principle: maximize probability to observe data given parameters:\n$$Y \\sim N(\\,\\beta_1X_1+\\beta_2X_2, \\sigma^2\\,) \\\\\n\\rm{L}\\,(\\,\\rm{Y} \\,|\\, \\beta_1,\\beta_2\\,) =\n\\frac{1}{\\sqrt{2\\pi\\sigma²}} \\exp^{-\\frac{(Y-\\beta_1X_1-\\beta_2X_2)^2}{2\\sigma²}}$$\n\n. . .\n\n* **Bayes theorem**: maximize posterior probability to observe parameters given data:\n$$P(\\rm{parameters} \\,|\\, \\rm{data})=\\frac{L(\\rm{data} \\,|\\, \\rm{parameters})*P(\\rm{parameters})}{\\int{L(\\rm{data} \\,|\\, \\rm{parameters})*P(\\rm{parameters}) \\, d(\\rm{parameters})}}$$\n\n. . .\n\n$$P(\\, \\beta_1,\\beta_2 \\,|\\, \\rm{Y}\\,) \\sim \\rm{L}\\,(\\,\\rm{Y} \\,|\\, \\beta_1,\\beta_2\\,)*P(\\beta_1,\\beta_2) \\sim \\exp^{-\\frac{(Y-\\beta_1X_1-\\beta_2X_2)^2}{2\\sigma²}}*\\exp^{-\\lambda(|\\beta_1|+|\\beta_2|)}$$\n$$-\\log{P(\\, \\beta_1,\\beta_2 \\,|\\, \\rm{Y}\\,)} \\sim (Y-\\beta_1X_1-\\beta_2X_2)^2 + \\lambda(|\\beta_1|+|\\beta_2|)$$\n\n. . .\n\n- LASSO is a form of Bayes theorem with Laplace prior\n\n## Moving from Statistics to Machine Learning\n\nA few peculiarities of data analysis that are typical for Machine Learning:\n\n- Very focused on **prediction** instead of biomarker discovery\n- Focus on **cross-validation** and regularizations\n- More **algorithmic** and less \"pen-and-paper\" derivations\n- More **Python** speaking than R speaking community\n\n![](assets/MachineLearning.jpg){width=70%}\n\n## Statistics vs. Machine Learning\n\n![](assets/Stats_vs_ML.jpg)\n\n## How does Machine Learning work?\n\nMachine Learning by default involves five basic steps:\n\n1. Split data set into **train**, **validation** and **test** subsets.\n2. Fit the model on the train subset.\n3. Validate your model on the validation subset.\n4. Repeat steps 1-3 a number of times and tune **hyperparameters**.\n5. Test the accuracy of the optimized model on the test subset.\n\n![](assets/TrainTestSplit.jpg){width=\"60%\"}\n\n## Toy Example of Machine Learning\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nN<-100\nx<-rnorm(N)\ny<-2*x+rnorm(N)\ndf<-data.frame(x,y)\nplot(y~x,data=df, col=\"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-7-1.png){width=576}\n:::\n:::\n\n\n## Toy Example: Train and Test Subsets\n\nWe randomly assign 70% of the data to training and 30% to test subsets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntrain<-df[sample(1:dim(df)[1],0.7*dim(df)[1]),]\ntest<-df[!rownames(df)%in%rownames(train),]\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\n## Toy Example: Validation of Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predicted<-as.numeric(predict(lm(y~x,data=train),newdata=test))\nplot(test$y~test_predicted,ylab=\"True y\",xlab=\"Pred y\",col=\"red\")\nabline(lm(test$y~test_predicted),col=\"darkgreen\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\n## Toy Example: Validation of Model (Cont.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(test$y~test_predicted))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = test$y ~ test_predicted)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80597 -0.78005  0.07636  0.52330  2.61924 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     0.02058    0.21588   0.095    0.925    \ntest_predicted  0.89953    0.08678  10.366 4.33e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.053 on 28 degrees of freedom\nMultiple R-squared:  0.7933,\tAdjusted R-squared:  0.7859 \nF-statistic: 107.4 on 1 and 28 DF,  p-value: 4.329e-11\n```\n:::\n:::\n\n\nThus the model explains 79% of variation on the test subset.\n\n## What is a Hyperparameter?\n\n- Hyperparameters are Machine Learning design parameters that are set before the learning process starts\n- For the toy model above, a hyperparameter can be e.g. the number of covariates to adjust the main variable x of interest for\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nfor(i in 1:10) { df[,paste0(\"PC\",i)]<-1*(1-i/10)*y+rnorm(N) }\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           x          y color        PC1        PC2        PC3        PC4\n1  0.5855288  1.3949830   red  0.6290309  0.4956198  1.3858900  1.7306635\n2  0.7094660  0.2627087   red  0.4200812  0.2522828  1.8727694 -0.8896729\n3 -0.1093033  0.2038119   red -0.6521979 -0.7478721  1.7292568  2.0936245\n4 -0.4534972 -2.2317496  blue -0.4132938 -1.6273709 -1.8931325 -1.7226819\n5  0.6058875  1.3528592  blue  1.5470811  0.4277027 -1.3382341  2.4658608\n6 -1.8179560 -4.1719599  blue -4.5752323 -1.5702807 -0.4227104 -0.9909633\n         PC5         PC6         PC7        PC8        PC9       PC10\n1  1.7719325  0.63529634  0.07742793 -0.4285716 -0.9474105 -1.5414026\n2  2.0270091 -0.19178516  1.58123715  2.0241138 -1.7998121  0.1943211\n3 -0.5010914 -1.10171748  0.58945128 -0.0492363  1.0156630  0.2644225\n4 -1.5067426 -0.88140715 -0.12733353 -0.4603672 -0.2350367 -1.1187352\n5  0.2602076  1.53274473  0.26918441 -0.8528851 -0.4643425  0.6509530\n6 -2.4616374 -0.07481652 -2.38832183 -2.1785221 -0.5951440 -1.0329002\n```\n:::\n:::\n\n\n## How does Cross-Validation work?\n\n- We should not include all PCs - this can be prone to overfitting\n- Cross-Validation is a way to combat overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain<-df[sample(1:dim(df)[1],0.6*dim(df)[1]),]\nval_test<-df[!rownames(df)%in%rownames(train),]\nvalidate<-val_test[sample(1:dim(val_test)[1],0.25*dim(val_test)[1]),]\ntest<-val_test[!rownames(val_test)%in%rownames(validate),]\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){width=576}\n:::\n:::\n\n\n## How does Cross-Validation work? (Cont.)\n\n- Fit linear regression model on training set and assess the error on validation set\n- Error: root mean square difference between y predicted by the trained model on the validation set, and the true y from the validation set\n- Looks like no dramatic decrease of RMSE after PC2\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n## Ultimate Model Evaluation\n\n- Thus optimal model is y~x+PC1+PC2\n- Perform final evaluation of the optimized / trained model on the test data set and report the final evaluation metric, adjusted R squared in our case\n- The model explains over 90% of variation on the unseen test data set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(predict(lm(y~x+PC1+PC2,data=train),newdata=test)~test$y))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = predict(lm(y ~ x + PC1 + PC2, data = train), newdata = test) ~ \n    test$y)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02409 -0.26361 -0.00061  0.19345  0.95801 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.22155    0.09485   2.336   0.0269 *  \ntest$y       0.93912    0.03994  23.514   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5002 on 28 degrees of freedom\nMultiple R-squared:  0.9518,\tAdjusted R-squared:  0.9501 \nF-statistic: 552.9 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## From Linear Models to ANNs\n\n- ANN: Mathematical algorithm / function Y = f(X) with special architecture\n- Highly non-linear dues to activation functions\n- Backward propagation for minimizing error\n\n![](assets/ANN.jpg){width=\"90%\"}\n\n## Universal Approximation Theorem\n\n![](assets/UAT.jpg)\n\n## Single Cells make Big Data\n\n![](assets/SingleCellBigData.jpg)\n\n## {background-image=\"../../assets/images/cover.jpg\"}\n\n### Thank you! Questions?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n         _                     \nplatform x86_64-conda-linux-gnu\nos       linux-gnu             \nmajor    4                     \nminor    2.2                   \n```\n:::\n:::\n\n\n[Graphics from [freepik](https://www.freepik.com), [Pexels](https://www.pexels.com)]{.small}  \n[2023 • [SciLifeLab](https://www.scilifelab.se/) • [NBIS](https://nbis.se/) • [RaukR](https://nbisweden.github.io/workshop-RaukR-2306/)]{.smaller}",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}