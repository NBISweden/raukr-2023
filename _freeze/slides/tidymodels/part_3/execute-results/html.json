{
  "hash": "ccdad327f2c582fd0d896ed468d6ef6b",
  "result": {
    "markdown": "---\ntitle: \"tidymodels - What makes a model?\"\nauthor: \"Max Kuhn\"\nimage: \"images/featured.png\"\nformat:\n  revealjs:\n    slide-number: true\n    code-line-numbers: true\n    footer: <https://nbisweden.github.io/raukr-2023>\n    include-before-body: styles/header.html\n    include-after-body: styles/footer-annotations.html\n    theme: [default, styles/tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk:\n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.align: \"center\"\n\nfig-format: svg\n---\n\n\n\n\n\n\n## The Data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n\ndata(cells, package = \"modeldata\")\ncells$case <- NULL\n\nset.seed(123)\ncell_split <- initial_split(cells, prop = 0.8, strata = class)\ncell_tr <- training(cell_split)\ncell_te <- testing(cell_split)\n```\n:::\n\n\n\n## Logistic Regresion\n\n*How do you fit a logistic model in R?*\n\n*How many different ways can you think of?*\n\n. . .\n\n\n-   `glm` for generalized linear model (e.g. logistic regression)\n\n-   `glmnet` for regularized regression\n\n-   `keras` for regression using TensorFlow\n\n-   `stan` for Bayesian regression\n\n-   `spark` for large data sets\n\n. . .\n\nThese all have the _same model equation_. \n\n## To specify a model \n\n. . .\n\n-   Choose a [model type]{.underline}\n-   Specify an engine\n-   Set the mode\n\n## To specify a model - choose a model type\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg()\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n:::\n\n\n\n. . .\n\n<br><br>\n\nA different model type (= different equation)\n\n. . .\n\n<br><br>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrand_forest()\n#> Random Forest Model Specification (unknown mode)\n#> \n#> Computational engine: ranger\n```\n:::\n\n\n\n:::notes\nModels have default engines\n:::\n\n## To specify a model \n\n-   Choose a model type\n-   Specify an [engine]{.underline}\n-   Set the mode\n\n## To specify a model - set the engine\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg() %>%\n  set_engine(\"glmnet\")\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glmnet\n```\n:::\n\n\n\n## To specify a model - set the engine\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic_reg() %>%\n  set_engine(\"stan\")\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: stan\n```\n:::\n\n\n\n## To specify a model \n\n-   Choose a model type\n-   Specify an engine\n-   Set the [mode]{.underline}\n\n\n## To specify a model - set the mode\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree()\n#> Decision Tree Model Specification (unknown mode)\n#> \n#> Computational engine: rpart\n```\n:::\n\n\n\n\n## To specify a model - set the mode\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndecision_tree() %>% \n  set_mode(\"classification\")\n#> Decision Tree Model Specification (classification)\n#> \n#> Computational engine: rpart\n```\n:::\n\n\n\n. . .\n\n<br></br>\n\nOther modes are \"regression\" and \"censored regresion\". \n\n. . .\n\n<br></br>\n\n\n\n\n::: r-fit-text\nAll available models are listed at <https://www.tidymodels.org/find/parsnip/> \n:::\n\n##  {background-iframe=\"https://www.tidymodels.org/find/parsnip/\"}\n\n::: footer\n:::\n\n\n## Models we'll be using today\n\n* Logistic regression\n* Decision trees\n\n## A single predictor\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/unnamed-chunk-9-1.svg){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## Logistic regression - a single predictor\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/logistic-line-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n:::\n:::\n\n## Logistic regression - a single predictor\n\n::: columns\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/logistic-line-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n-   Outcome modeled as linear combination of predictors:\n\n$log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x + \\epsilon$\n\n- Find a line that maximizes the binomial (log-)likelihood function. \n\n:::\n:::\n\n## Decision trees - a single predictor\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/unnamed-chunk-13-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n:::\n:::\n\n## Decision trees - a single predictor\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/unnamed-chunk-14-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n-   Series of splits or if/then statements based on predictors\n\n-   First the tree *grows* until some condition is met (maximum depth, no more data)\n\n-   Then the tree is *pruned* to reduce its complexity\n:::\n:::\n\n## Decision trees - a single predictor\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/unnamed-chunk-15-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/tree-line-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n## All models are wrong, but some are useful!\n\n::: columns\n::: {.column width=\"50%\"}\n### Logistic regression\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/logistic-line-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n### Decision trees\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_3_files/figure-html/tree-line-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n# A model workflow\n\n## Workflows bind preprocessors and models\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/good_workflow.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n:::notes\nExplain that PCA that is a preprocessor / dimensionality reduction, used to decorrelate data\n:::\n\n\n## What is wrong with this? {.annotation}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/bad_workflow.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Why a `workflow()`? \n\n. . .\n\n-   Workflows handle new data better than base R tools in terms of new factor levels\n\n. . .\n\n-   You can use other preprocessors besides formulas (more on feature engineering)\n\n. . .\n\n-   They can help organize your work when working with multiple models\n\n. . .\n\n-   [Most importantly]{.underline}, a workflow captures the entire modeling process: `fit()` and `predict()` apply to the preprocessing steps in addition to the actual model fit\n\n::: notes\nTwo ways workflows handle levels better than base R:\n\n-   Enforces that new levels are not allowed at prediction time (this is an optional check that can be turned off)\n\n-   Restores missing levels that were present at fit time, but happen to be missing at prediction time (like, if your \"new\" data just doesn't have an instance of that level)\n:::\n\n## A model workflow \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"classification\")\n\ntree_spec %>% \n  fit(class ~ ., data = cell_tr) \n#> parsnip model object\n#> \n#> n= 1615 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 1615 575 PS (0.64396285 0.35603715)  \n#>     2) total_inten_ch_2< 41713.5 687  40 PS (0.94177584 0.05822416) *\n#>     3) total_inten_ch_2>=41713.5 928 393 WS (0.42349138 0.57650862)  \n#>       6) fiber_width_ch_1< 11.35657 427 160 PS (0.62529274 0.37470726)  \n#>        12) var_inten_ch_1< 199.704 382 122 PS (0.68062827 0.31937173)  \n#>          24) kurt_inten_ch_1>=-0.3456671 270  62 PS (0.77037037 0.22962963) *\n#>          25) kurt_inten_ch_1< -0.3456671 112  52 WS (0.46428571 0.53571429)  \n#>            50) total_inten_ch_1< 13594 31   6 PS (0.80645161 0.19354839) *\n#>            51) total_inten_ch_1>=13594 81  27 WS (0.33333333 0.66666667)  \n#>             102) diff_inten_density_ch_4>=190.8823 7   0 PS (1.00000000 0.00000000) *\n#>             103) diff_inten_density_ch_4< 190.8823 74  20 WS (0.27027027 0.72972973) *\n#>        13) var_inten_ch_1>=199.704 45   7 WS (0.15555556 0.84444444) *\n#>       7) fiber_width_ch_1>=11.35657 501 126 WS (0.25149701 0.74850299)  \n#>        14) eq_ellipse_oblate_vol_ch_1>=1673.942 35  12 PS (0.65714286 0.34285714) *\n#>        15) eq_ellipse_oblate_vol_ch_1< 1673.942 466 103 WS (0.22103004 0.77896996) *\n```\n:::\n\n\n\n## A model workflow \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"classification\")\n\nworkflow() %>%\n  add_formula(class ~ .) %>%\n  add_model(tree_spec) %>%\n  fit(data = cell_tr) \n#> ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> class ~ .\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> n= 1615 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 1615 575 PS (0.64396285 0.35603715)  \n#>     2) total_inten_ch_2< 41713.5 687  40 PS (0.94177584 0.05822416) *\n#>     3) total_inten_ch_2>=41713.5 928 393 WS (0.42349138 0.57650862)  \n#>       6) fiber_width_ch_1< 11.35657 427 160 PS (0.62529274 0.37470726)  \n#>        12) var_inten_ch_1< 199.704 382 122 PS (0.68062827 0.31937173)  \n#>          24) kurt_inten_ch_1>=-0.3456671 270  62 PS (0.77037037 0.22962963) *\n#>          25) kurt_inten_ch_1< -0.3456671 112  52 WS (0.46428571 0.53571429)  \n#>            50) total_inten_ch_1< 13594 31   6 PS (0.80645161 0.19354839) *\n#>            51) total_inten_ch_1>=13594 81  27 WS (0.33333333 0.66666667)  \n#>             102) diff_inten_density_ch_4>=190.8823 7   0 PS (1.00000000 0.00000000) *\n#>             103) diff_inten_density_ch_4< 190.8823 74  20 WS (0.27027027 0.72972973) *\n#>        13) var_inten_ch_1>=199.704 45   7 WS (0.15555556 0.84444444) *\n#>       7) fiber_width_ch_1>=11.35657 501 126 WS (0.25149701 0.74850299)  \n#>        14) eq_ellipse_oblate_vol_ch_1>=1673.942 35  12 PS (0.65714286 0.34285714) *\n#>        15) eq_ellipse_oblate_vol_ch_1< 1673.942 466 103 WS (0.22103004 0.77896996) *\n```\n:::\n\n\n\n## A model workflow \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"classification\")\n\nworkflow(class ~ ., tree_spec) %>% \n  fit(data = cell_tr) \n#> ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: decision_tree()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> class ~ .\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> n= 1615 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 1615 575 PS (0.64396285 0.35603715)  \n#>     2) total_inten_ch_2< 41713.5 687  40 PS (0.94177584 0.05822416) *\n#>     3) total_inten_ch_2>=41713.5 928 393 WS (0.42349138 0.57650862)  \n#>       6) fiber_width_ch_1< 11.35657 427 160 PS (0.62529274 0.37470726)  \n#>        12) var_inten_ch_1< 199.704 382 122 PS (0.68062827 0.31937173)  \n#>          24) kurt_inten_ch_1>=-0.3456671 270  62 PS (0.77037037 0.22962963) *\n#>          25) kurt_inten_ch_1< -0.3456671 112  52 WS (0.46428571 0.53571429)  \n#>            50) total_inten_ch_1< 13594 31   6 PS (0.80645161 0.19354839) *\n#>            51) total_inten_ch_1>=13594 81  27 WS (0.33333333 0.66666667)  \n#>             102) diff_inten_density_ch_4>=190.8823 7   0 PS (1.00000000 0.00000000) *\n#>             103) diff_inten_density_ch_4< 190.8823 74  20 WS (0.27027027 0.72972973) *\n#>        13) var_inten_ch_1>=199.704 45   7 WS (0.15555556 0.84444444) *\n#>       7) fiber_width_ch_1>=11.35657 501 126 WS (0.25149701 0.74850299)  \n#>        14) eq_ellipse_oblate_vol_ch_1>=1673.942 35  12 PS (0.65714286 0.34285714) *\n#>        15) eq_ellipse_oblate_vol_ch_1< 1673.942 466 103 WS (0.22103004 0.77896996) *\n```\n:::\n\n\n\n\n## Predict with your model \n\nHow do you use your new `tree_fit` model?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_spec <-\n  decision_tree() %>% \n  set_mode(\"classification\")\n\ntree_fit <-\n  workflow(class ~ ., tree_spec) %>% \n  fit(data = cell_tr) \n\npredict(tree_fit, cell_te %>% slice(1:6), type = \"prob\")\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|  .pred_PS|  .pred_WS|\n|---------:|---------:|\n| 0.9417758| 0.0582242|\n| 0.2210300| 0.7789700|\n| 0.2210300| 0.7789700|\n| 0.9417758| 0.0582242|\n| 0.2210300| 0.7789700|\n| 0.9417758| 0.0582242|\n\n</div>\n:::\n:::\n\n\n\n\n# The tidymodels prediction guarantee!\n\n. . .\n\n-   The predictions will always be inside a **tibble**\n-   The column names and types are **unsurprising** and **predictable**\n-   The number of rows in `new_data` and the output **are the same**\n\n\n\n\n\n",
    "supporting": [
      "part_3_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}