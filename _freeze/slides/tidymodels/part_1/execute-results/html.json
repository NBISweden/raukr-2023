{
  "hash": "73aed967081dc612d88ed5e90d8a031d",
  "result": {
    "markdown": "---\ntitle: \"tidymodels - introduction\"\nauthor: \"Max Kuhn\"\n\nformat:\n  revealjs:\n    slide-number: true\n    code-line-numbers: true\n    footer: <https://nbisweden.github.io/raukr-2023/>\n    include-before-body: styles/header.html\n    include-after-body: styles/footer-annotations.html\n    theme: [default, styles/tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk:\n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.align: \"center\"\n\nfig-format: svg\n---\n\n\n\n## About me\n\n - Becton-Dickinson (6y): molecular diagnostics for infectious diseases, non-clinical and clinical\n - Pfizer (12y): nonclinical, Med chem, Comp {bio,chem} support\n - <span style=\"color:LightGray;\"><strike>RStudio</strike></span> posit PBC (>= 2016): modeling packages\n \nSelected R packages: [`caret`](https://topepo.github.io/caret/), [`C50`](https://topepo.github.io/C5.0/), [`Cubist`](https://topepo.github.io/Cubist/), a lot of [tidymodels](https://github.com/orgs/tidymodels/repositories)\n\n - [_Applied Predictive Modeling_](http://appliedpredictivemodeling.com)\n - [_Feature Engineering and Selection_](https://bookdown.org/max/FES)\n - [_Tidy Models with R_](http://tmwr.org)\n - [_Nonclinical Statistics for Pharmaceutical and Biotechnology Industries_](https://link.springer.com/book/10.1007/978-3-319-23558-5) (ed, auth) \n \n## Some basic advice\n\n - ML models are really bad at determining/quantifying associations and evaluating hypotheses\n - You almost certainly don't need deep learning unless you have a ton of images. \n - Basic statistic (e.g. ANOVA, linear regression, etc) go a long way. \n - Clustering is over-rated and over-used. \n - Analyze your measurement systems. \n - Always have a data set that _could_ contradict what you think that you know. \n - The only way to be comfortable with your data is to never look at them. \n \n## The Whole Game\n\nWe often begin with a very high-level view of a data analysis project from start to finish. \n\nThe goal is to give you a sense of strategy instead of a series of tactics. \n\nI'll focus on predictive modeling (aka machine learning) on some biological data. \n\n## Cell Segmentation Data\n\n\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n[Hill, LaPan, Li and Haney (2007)](http://www.biomedcentral.com/1471-2105/8/340) develop models to predict which cells in a high content screen were well segmented. \n\nThe data consists of 56 imaging measurements on 2019 cells.\n\nThe outcome class is contained in a factor variable called `class` with levels `PS` for poorly segmented and `WS` for well segmented.\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/cells.png){fig-align='center' width=80%}\n:::\n:::\n\n\n:::\n\n::::\n\n## What is the goal\n\nThe authors ran a high-content screening lab and were looking for a tool to: \n\n * validate/characterize different image analysis parameters. \n * pre-filter cells prior to analysis/modeling. \n \nSo their goal is to do their best to predict bad image data. \n\nThere's not much inference/understanding that is required; be as accurate as possible.\n\n\n\n\n\n\n## Data Usage\n\nWe will immediately split the data into _training_ and _testing_ sets (75% / 25%). \n\nWhat are some interesting features of these data? We'll only use our training set to investigate. \n\n## Some key features\n\n:::: {.columns}\n\n\n::: {.column width=\"50%\"}\n\nclass imbalance\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-imbal-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\nClusters of highly correlated features\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-cor-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n:::\n\n::::\n\n\n## Feature Engineering\n\nThis describes the process of having good representations of your predictors so that the model has the easiest possible time fitting the data. \n\nFor the multicollinearity, we could: \n\n* Do nothing and see how that works\n* Maybe try feature extraction (e.g. [PCA, PLS, and similar](https://bookdown.org/max/FES/numeric-many-to-many.html))\n* Filter out predictions to reduce correlations\n* Use a model that is resistant to multicollinearity. \n\n## How will we measure performance\n\nWe have to classes; there are [a lot of options](https://bookdown.org/max/FES/measuring-performance.html#class-metrics):\n\n* Accuracy and similar.\n* ROC and PR curves\n* Brier scores\n\nWe'll compute all of these but will focus on Brier scores\n\n## The process of building a model\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](images/process.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n## Logistic regression\n\nI usually start simple. Logistic regression is a commonly used model (both for prediction and inference). \n\nWe'll try different versions of logistic regression as well as with different feature representations. \n\nTo get good measures of performance, we will use [resampling methods](https://bookdown.org/max/FES/resampling.html) to compute our results. \n\n\n\n\n\n## Logistic regression results\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-logistic-ranking-1.svg){fig-align='center' width=90%}\n:::\n:::\n\n\n## Filtering results\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-logistic-filters-1.svg){fig-align='center' width=70%}\n:::\n:::\n\nThe best model retained 49 predictors out of 56. \n\n## Partial Least Squares Results\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-logistic-pls-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Evaluating More Models\n\nAt this point we would try different models too (e.g. boosting, etc). \n\nFrom brevity, we'll stop here to illustrate what occurs next. \n\nLet's choose a PLS logistic model as our \"final model.\"\n\nHow does this fit work with the test set? \n\n. . .\n\nThe Brier score was 0.126. \n\n## Test Set Predictions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-logistic-hist-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n\n## Test Set Calibration\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/cell-logistic-cal-1.svg){fig-align='center' width=50%}\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}