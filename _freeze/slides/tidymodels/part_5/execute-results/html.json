{
  "hash": "f28812d5bcedd43a9c4502703f7fa3e4",
  "result": {
    "markdown": "---\ntitle: \"tidymodels - Feature engineering\"\nauthor: \"Max Kuhn\"\nimage: \"images/featured.png\"\nformat:\n  revealjs:\n    slide-number: true\n    code-line-numbers: true\n    footer: <https://nbisweden.github.io/raukr-2023>\n    include-before-body: styles/header.html\n    include-after-body: styles/footer-annotations.html\n    theme: [default, styles/tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk:\n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.align: \"center\"\n\nfig-format: svg\n---\n\n\n## Previously...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(doParallel)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ncl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))\nregisterDoParallel(cl)\n\ndata(cells, package = \"modeldata\")\ncells$case <- NULL\n\nset.seed(123)\ncell_split <- initial_split(cells, prop = 0.8, strata = class)\ncell_tr <- training(cell_split)\ncell_te <- testing(cell_split)\n\nset.seed(123)\ncell_rs <- vfold_cv(cell_tr, v = 10, strata = class)\n\ncls_metrics <- metric_set(brier_class, roc_auc, kap)\n```\n:::\n\n\n\n## Working with our predictors\n\nWe might want to modify our predictors columns for a few reasons: \n\n::: {.incremental}\n- The model requires them in a different format (e.g. dummy variables for `lm()`).\n- The model needs certain data qualities (e.g. same units for K-NN).\n- The outcome is better predicted when one or more columns are transformed in some way (a.k.a \"feature engineering\"). \n:::\n\n. . .\n\nThe first two reasons are fairly predictable ([next page](https://www.tmwr.org/pre-proc-table.html#tab:preprocessing)).\n\nThe last one depends on your modeling problem. \n\n\n##  {background-iframe=\"https://www.tmwr.org/pre-proc-table.html#tab:preprocessing\"}\n\n::: footer\n:::\n\n\n## What is feature engineering?\n\nThink of a feature as some *representation* of a predictor that will be used in a model.\n\n. . .\n\nExample representations:\n\n-   Interactions\n-   Polynomial expansions/splines\n-   PCA feature extraction\n\nThere are a lot of examples in [_Feature Engineering and Selection_](https://bookdown.org/max/FES/).\n\n\n\n## Example: Dates\n\nHow can we represent date columns for our model?\n\n. . .\n\nWhen a date column is used in its native format, it is usually converted by an R model to an integer.\n\n. . .\n\nIt can be re-engineered as:\n\n-   Days since a reference date\n-   Day of the week\n-   Month\n-   Year\n-   Indicators for holidays\n\n::: notes\nThe main point is that we try to maximize performance with different versions of the predictors. \n\nMention that, for the Chicago data, the day or the week features are usually the most important ones in the model.\n:::\n\n## General definitions \n\n-   *Data preprocessing* steps allow your model to fit.\n\n-   *Feature engineering* steps help the model do the least work to predict the outcome as well as possible.\n\nThe recipes package can handle both!\n\nIn a little bit, we'll see successful (and unsuccessful) feature engineering methods for our example data. \n\n\n::: notes\nThese terms are often used interchangeably in the ML community but we want to distinguish them.\n:::\n\n\n## Prepare your data for modeling\n\n- The recipes package is an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\n. . .\n\n- Statistical parameters for the steps can be _estimated_ from an initial data set and then _applied_ to other data sets.\n\n. . .\n\n- The resulting processed output can be used as inputs for statistical or machine learning models.\n\n## A first recipe \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncell_rec <- \n  recipe(class ~ ., data = cell_tr) \n```\n:::\n\n\n. . .\n\n- The `recipe()` function assigns columns to roles of \"outcome\" or \"predictor\" using the formula\n\n## A first recipe \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(cell_rec)\n#> # A tibble: 57 × 4\n#>    variable                     type      role      source  \n#>    <chr>                        <list>    <chr>     <chr>   \n#>  1 angle_ch_1                   <chr [2]> predictor original\n#>  2 area_ch_1                    <chr [2]> predictor original\n#>  3 avg_inten_ch_1               <chr [2]> predictor original\n#>  4 avg_inten_ch_2               <chr [2]> predictor original\n#>  5 avg_inten_ch_3               <chr [2]> predictor original\n#>  6 avg_inten_ch_4               <chr [2]> predictor original\n#>  7 convex_hull_area_ratio_ch_1  <chr [2]> predictor original\n#>  8 convex_hull_perim_ratio_ch_1 <chr [2]> predictor original\n#>  9 diff_inten_density_ch_1      <chr [2]> predictor original\n#> 10 diff_inten_density_ch_3      <chr [2]> predictor original\n#> # ℹ 47 more rows\n```\n:::\n\n\n## Transforming individual predictors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\ncell_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors())\n```\n:::\n\n\n. . .\n\nThe YJ transformation can be used to produce more symmetric distirbutions for predictors. It is very similar to the Box-Cox transformation. \n\n\n## Standardize predictors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\npca_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  step_normalize(all_predictors())\n```\n:::\n\n\n. . .\n\n- This centers and scales the numeric predictors.\n\n\n- The recipe will use the _training_ set to estimate the means and standard deviations of the data.\n\n. . .\n\n- All data the recipe is applied to will be normalized using those statistics (there is no re-estimation).\n\n## Convert the data to PCA components\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\npca_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), num_comp = 10)\n```\n:::\n\n\n\n## Convert the data to PLS components\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\npca_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pls(all_predictors(), outcome = vars(class), num_comp = 10)\n```\n:::\n\n\n. . .\n\nSince PLS is supervised, we have to use the `outcome` argument. \n\n\n## Reduce correlation \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nfilter_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  step_corr(all_numeric_predictors(), threshold = 0.9)\n```\n:::\n\n\n. . .\n\nTo deal with highly correlated predictors, find the minimum set of predictor columns that make the pairwise correlations less than the threshold.\n\n\n\n## Using a workflow\n\n\n::: {.cell layout-align=\"center\" hash='part_5_cache/revealjs/unnamed-chunk-6_75326bff2500107a747f4c7aa77f8a87'}\n\n```{.r .cell-code}\ncell_pca_wflow <-\n  workflow() %>%\n  add_recipe(pca_rec) %>%\n  add_model(logistic_reg())\n \nctrl <- control_resamples(save_pred = TRUE)\n\nset.seed(9)\ncell_glm_res <-\n  cell_pca_wflow %>%\n  fit_resamples(cell_rs, control = ctrl, metrics = cls_metrics)\n\ncollect_metrics(cell_glm_res)\n#> # A tibble: 3 × 6\n#>   .metric     .estimator  mean     n std_err .config             \n#>   <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 brier_class binary     0.138    10 0.00443 Preprocessor1_Model1\n#> 2 kap         binary     0.559    10 0.0171  Preprocessor1_Model1\n#> 3 roc_auc     binary     0.871    10 0.00877 Preprocessor1_Model1\n```\n:::\n\n\n\n## Recipes are estimated \n\nPreprocessing steps in a recipe use the *training set* to compute quantities.\n\n. . .\n\nWhat kind of quantities are computed for preprocessing?\n\n-   Levels of a factor\n-   Whether a column has zero variance\n-   Normalization\n-   Feature extraction\n-   Effect encodings\n\n. . .\n\nWhen a recipe is part of a workflow, this estimation occurs when `fit()` is called.\n\n_The recipe is estimated within each resample_.\n\n## Getting specific results\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncell_pca_fit <-\n  cell_pca_wflow %>% \n  fit(data = cell_tr)\n\ncell_pca_fit %>% \n  extract_recipe() %>% \n  tidy(number = 1)\n#> # A tibble: 52 × 3\n#>    terms                    value id              \n#>    <chr>                    <dbl> <chr>           \n#>  1 angle_ch_1               0.787 YeoJohnson_J3XdN\n#>  2 area_ch_1               -0.923 YeoJohnson_J3XdN\n#>  3 avg_inten_ch_1          -0.337 YeoJohnson_J3XdN\n#>  4 avg_inten_ch_2           0.425 YeoJohnson_J3XdN\n#>  5 avg_inten_ch_3           0.200 YeoJohnson_J3XdN\n#>  6 avg_inten_ch_4           0.220 YeoJohnson_J3XdN\n#>  7 diff_inten_density_ch_1 -0.937 YeoJohnson_J3XdN\n#>  8 diff_inten_density_ch_3  0.103 YeoJohnson_J3XdN\n#>  9 diff_inten_density_ch_4  0.123 YeoJohnson_J3XdN\n#> 10 entropy_inten_ch_1      -0.440 YeoJohnson_J3XdN\n#> # ℹ 42 more rows\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncell_pca_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n#> # A tibble: 11 × 5\n#>    term        estimate std.error statistic  p.value\n#>    <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#>  1 (Intercept)  -1.08      0.0804   -13.4   9.76e-41\n#>  2 PC01          0.426     0.0245    17.4   6.02e-68\n#>  3 PC02          0.202     0.0228     8.85  8.61e-19\n#>  4 PC03          0.362     0.0277    13.1   4.44e-39\n#>  5 PC04         -0.103     0.0334    -3.07  2.16e- 3\n#>  6 PC05         -0.242     0.0422    -5.72  1.04e- 8\n#>  7 PC06         -0.145     0.0443    -3.27  1.08e- 3\n#>  8 PC07          0.132     0.0539     2.45  1.42e- 2\n#>  9 PC08         -0.0348    0.0499    -0.699 4.85e- 1\n#> 10 PC09         -0.0438    0.0610    -0.718 4.72e- 1\n#> 11 PC10          0.104     0.0676     1.53  1.25e- 1\n```\n:::\n\n:::\n\n::::\n\n\n\n\n\n## Debugging a recipe\n\n- Typically, you will want to use a workflow to estimate and apply a recipe.\n\n. . .\n\n- If you have an error and need to debug your recipe, the original recipe object (e.g. `pca_rec`) can be estimated manually with a function called `prep()`. It is analogous to `fit()`. See [TMwR section 16.4](https://www.tmwr.org/dimensionality.html#recipe-functions)\n\n. . .\n\n- Another function (`bake()`) is analogous to `predict()`, and gives you the processed data back.\n\n. . .\n\n- The `tidy()` function can be used to get specific results from the recipe.\n\n## More on recipes\n\n-   Once `fit()` is called on a workflow, changing the model does not re-fit the recipe.\n\n. . .\n\n-   A list of all known steps is at <https://www.tidymodels.org/find/recipes/>.\n\n. . .\n\n-   Some steps can be [skipped](https://recipes.tidymodels.org/articles/Skipping.html) when using `predict()`.\n\n. . .\n\n-   The [order](https://recipes.tidymodels.org/articles/Ordering.html) of the steps matters.\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}