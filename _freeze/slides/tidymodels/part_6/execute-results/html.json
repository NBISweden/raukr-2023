{
  "hash": "c78df34a16ba35de8d36f44c9f38d167",
  "result": {
    "markdown": "---\ntitle: \"tidymodels - Tuning Hyperparameters\"\nauthor: \"Max Kuhn\"\n\nformat:\n  revealjs:\n    slide-number: true\n    code-line-numbers: true\n    footer: <https://nbisweden.github.io/raukr-2023>\n    include-before-body: styles/header.html\n    include-after-body: styles/footer-annotations.html\n    theme: [default, styles/tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk:\n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.align: \"center\"\n\nfig-format: svg\n---\n\n\n## Previously...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(doParallel)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ncl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))\nregisterDoParallel(cl)\n\ndata(cells, package = \"modeldata\")\ncells$case <- NULL\n\nset.seed(123)\ncell_split <- initial_split(cells, prop = 0.8, strata = class)\ncell_tr <- training(cell_split)\ncell_te <- testing(cell_split)\n\nset.seed(123)\ncell_rs <- vfold_cv(cell_tr, v = 10, strata = class)\n\ncls_metrics <- metric_set(brier_class, roc_auc, kap)\n```\n:::\n\n\n\n## Tuning parameters\n\nSome model or preprocessing parameters cannot be estimated directly from the data.\n\n. . .\n\nSome examples:\n\n-   Tree depth in decision trees\n-   Number of neighbors in a K-nearest neighbor model\n\n# Activation function in neural networks?\n\nSigmoidal functions, ReLu, etc.\n\n::: fragment\nYes, it is a tuning parameter.\nâœ…\n:::\n\n# Number of PCA components to retain?\n\n::: fragment\nYes, it is a tuning parameter.\nâœ…\n:::\n\n# Bayesian priors for model parameters?\n\n::: fragment\nHmmmm, probably not.\nThese are based on prior belief.\nâŒ\n:::\n\n\n\n# Is the random seed a tuning parameter?\n\n::: fragment\nNope. It is not. \nâŒ\n:::\n\n## Optimize tuning parameters\n\n-   Try different values and measure their performance.\n\n. . .\n\n-   Find good values for these parameters.\n\n. . .\n\n-   Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n## Optimize tuning parameters\n\nThe main two strategies for optimization are:\n\n. . .\n\n-   **Grid search** ðŸ’  which tests a pre-defined set of candidate values ([_TMwR_](https://www.tmwr.org/grid-search.html))\n\n-   **Iterative search** ðŸŒ€ which suggests/estimates new values of candidate parameters to evaluate ([_TMwR_](https://www.tmwr.org/iterative-search.html))\n\n## Choosing tuning parameters\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"4,7\"}\ncell_rec <- \n  recipe(class ~ ., data = cell_tr) %>% \n  step_YeoJohnson(all_predictors()) %>% \n  step_normalize(all_predictors())\n\nglmn_spec <- \n  logistic_reg(penalty = tune(), mixture = tune()) %>% \n  set_engine(\"glmnet\")\n\nglmn_wflow <- workflow(cell_rec, glmn_spec)\n```\n:::\n\n\n\n## Grid search\n\n#### Parameters\n\n-   The tidymodels framework provides pre-defined information on tuning parameters (such as their type, range, transformations, etc).\n\n-   The `extract_parameter_set_dials()` function extracts these tuning parameters and the info.\n\n::: fragment\n#### Grids\n\n-   Create your grid manually or automatically.\n\n-   The `grid_*()` functions can make a grid.\n:::\n\n::: notes\nMost basic (but very effective) way to tune models\n:::\n\n## Create a grid \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglmn_wflow %>% \n  extract_parameter_set_dials()\n#> Collection of 2 parameters for tuning\n#> \n#>  identifier    type    object\n#>     penalty penalty nparam[+]\n#>     mixture mixture nparam[+]\n```\n:::\n\n\n. . .\n\nA parameter set can be updated (e.g. to change the ranges).\n\n. . .\n\nThere are also functions for each parameter:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenalty()\n#> Amount of Regularization (quantitative)\n#> Transformer: log-10 [1e-100, Inf]\n#> Range (transformed scale): [-10, 0]\n```\n:::\n\n\n## Create a grid \n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(99)\nglmn_grid <- \n  glmn_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_latin_hypercube(size = 25)\n\nglmn_grid\n#> # A tibble: 25 Ã— 2\n#>          penalty mixture\n#>            <dbl>   <dbl>\n#>  1 0.00000000644  0.544 \n#>  2 0.000404       0.336 \n#>  3 0.000832       0.691 \n#>  4 0.0000000293   0.226 \n#>  5 0.0114         0.836 \n#>  6 0.855          0.120 \n#>  7 0.00705        0.537 \n#>  8 0.0000604      0.362 \n#>  9 0.0000000216   0.0548\n#> 10 0.0000320      0.290 \n#> # â„¹ 15 more rows\n```\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n::: fragment\n-   A *space-filling design* like this tends to perform better than random grids.\n-   Space-filling designs are also usually more efficient than regular grids.\n:::\n:::\n:::\n\n## Create a grid\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nset.seed(99)\nglmn_grid <- \n  glmn_wflow %>% \n  extract_parameter_set_dials() %>% \n  grid_regular(levels = c(4, 4))\n\nglmn_grid\n#> # A tibble: 16 Ã— 2\n#>         penalty mixture\n#>           <dbl>   <dbl>\n#>  1 0.0000000001   0.05 \n#>  2 0.000000215    0.05 \n#>  3 0.000464       0.05 \n#>  4 1              0.05 \n#>  5 0.0000000001   0.367\n#>  6 0.000000215    0.367\n#>  7 0.000464       0.367\n#>  8 1              0.367\n#>  9 0.0000000001   0.683\n#> 10 0.000000215    0.683\n#> 11 0.000464       0.683\n#> 12 1              0.683\n#> 13 0.0000000001   1    \n#> 14 0.000000215    1    \n#> 15 0.000464       1    \n#> 16 1              1\n```\n:::\n\n\n## Update parameter ranges\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5-6\"}\nset.seed(99)\nglmn_grid <- \n  glmn_wflow %>% \n  extract_parameter_set_dials() %>% \n  update(penalty = penalty(c(-5, -1))) %>% \n  grid_latin_hypercube(size = 25)\n\nglmn_grid\n#> # A tibble: 25 Ã— 2\n#>      penalty mixture\n#>        <dbl>   <dbl>\n#>  1 0.0000529  0.544 \n#>  2 0.00439    0.336 \n#>  3 0.00586    0.691 \n#>  4 0.0000970  0.226 \n#>  5 0.0167     0.836 \n#>  6 0.0939     0.120 \n#>  7 0.0138     0.537 \n#>  8 0.00205    0.362 \n#>  9 0.0000858  0.0548\n#> 10 0.00159    0.290 \n#> # â„¹ 15 more rows\n```\n:::\n\n\n## The results \n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nglmn_grid %>% \n  ggplot(aes(penalty, mixture)) +\n  geom_point(size = 4) +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/show-grid-code-1.svg){fig-align='center'}\n:::\n:::\n\n\n# Use the `tune_*()` functions to tune models\n\n## glmnet grid search \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_grid(save_pred = TRUE, parallel_over = \"everything\")\n\nglmn_res <-\n  glmn_wflow %>%\n  tune_grid(\n    resamples = cell_rs,\n    grid = glmn_grid,\n    control = ctrl,\n    metrics = cls_metrics\n  )\n```\n:::\n\n\n::: notes\n-   `tune_grid()` is representative of tuning function syntax\n-   similar to `fit_resamples()`\n:::\n\n\n## glmnet grid search \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglmn_res\n#> # Tuning results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 Ã— 5\n#>    splits             id     .metrics          .notes           .predictions\n#>    <list>             <chr>  <list>            <list>           <list>      \n#>  1 <split [1453/162]> Fold01 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  2 <split [1453/162]> Fold02 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  3 <split [1453/162]> Fold03 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  4 <split [1453/162]> Fold04 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  5 <split [1453/162]> Fold05 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  6 <split [1454/161]> Fold06 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  7 <split [1454/161]> Fold07 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  8 <split [1454/161]> Fold08 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#>  9 <split [1454/161]> Fold09 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>    \n#> 10 <split [1454/161]> Fold10 <tibble [75 Ã— 6]> <tibble [0 Ã— 3]> <tibble>\n```\n:::\n\n\n\n## Grid results \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(glmn_res)\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/autoplot-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Tuning results \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(glmn_res)\n#> # A tibble: 75 Ã— 8\n#>      penalty mixture .metric     .estimator  mean     n std_err .config         \n#>        <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>           \n#>  1 0.0000858  0.0548 brier_class binary     0.127    10 0.00310 Preprocessor1_Mâ€¦\n#>  2 0.0000858  0.0548 kap         binary     0.596    10 0.0137  Preprocessor1_Mâ€¦\n#>  3 0.0000858  0.0548 roc_auc     binary     0.892    10 0.00533 Preprocessor1_Mâ€¦\n#>  4 0.0939     0.120  brier_class binary     0.138    10 0.00322 Preprocessor1_Mâ€¦\n#>  5 0.0939     0.120  kap         binary     0.559    10 0.0157  Preprocessor1_Mâ€¦\n#>  6 0.0939     0.120  roc_auc     binary     0.875    10 0.00738 Preprocessor1_Mâ€¦\n#>  7 0.0376     0.152  brier_class binary     0.134    10 0.00332 Preprocessor1_Mâ€¦\n#>  8 0.0376     0.152  kap         binary     0.562    10 0.0122  Preprocessor1_Mâ€¦\n#>  9 0.0376     0.152  roc_auc     binary     0.880    10 0.00697 Preprocessor1_Mâ€¦\n#> 10 0.000793   0.180  brier_class binary     0.127    10 0.00309 Preprocessor1_Mâ€¦\n#> # â„¹ 65 more rows\n```\n:::\n\n\n## Tuning results \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(glmn_res, summarize = FALSE)\n#> # A tibble: 750 Ã— 7\n#>    id       penalty mixture .metric     .estimator .estimate .config            \n#>    <chr>      <dbl>   <dbl> <chr>       <chr>          <dbl> <chr>              \n#>  1 Fold01 0.0000858  0.0548 kap         binary         0.517 Preprocessor1_Modeâ€¦\n#>  2 Fold01 0.0000858  0.0548 brier_class binary         0.143 Preprocessor1_Modeâ€¦\n#>  3 Fold01 0.0000858  0.0548 roc_auc     binary         0.867 Preprocessor1_Modeâ€¦\n#>  4 Fold02 0.0000858  0.0548 kap         binary         0.541 Preprocessor1_Modeâ€¦\n#>  5 Fold02 0.0000858  0.0548 brier_class binary         0.130 Preprocessor1_Modeâ€¦\n#>  6 Fold02 0.0000858  0.0548 roc_auc     binary         0.892 Preprocessor1_Modeâ€¦\n#>  7 Fold03 0.0000858  0.0548 kap         binary         0.618 Preprocessor1_Modeâ€¦\n#>  8 Fold03 0.0000858  0.0548 brier_class binary         0.118 Preprocessor1_Modeâ€¦\n#>  9 Fold03 0.0000858  0.0548 roc_auc     binary         0.908 Preprocessor1_Modeâ€¦\n#> 10 Fold04 0.0000858  0.0548 kap         binary         0.621 Preprocessor1_Modeâ€¦\n#> # â„¹ 740 more rows\n```\n:::\n\n\n## Choose a parameter combination \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_best(glmn_res, metric = \"brier_class\")\n#> # A tibble: 5 Ã— 8\n#>     penalty mixture .metric     .estimator  mean     n std_err .config          \n#>       <dbl>   <dbl> <chr>       <chr>      <dbl> <int>   <dbl> <chr>            \n#> 1 0.0000529   0.544 brier_class binary     0.126    10 0.00317 Preprocessor1_Moâ€¦\n#> 2 0.000183    0.948 brier_class binary     0.126    10 0.00313 Preprocessor1_Moâ€¦\n#> 3 0.0000177   0.477 brier_class binary     0.126    10 0.00317 Preprocessor1_Moâ€¦\n#> 4 0.0000388   0.404 brier_class binary     0.126    10 0.00315 Preprocessor1_Moâ€¦\n#> 5 0.0000227   0.633 brier_class binary     0.126    10 0.00318 Preprocessor1_Moâ€¦\n```\n:::\n\n\n## Tuning a recipe\n\nNow let's fit an \"ordinary\" logistic regression and tune the number of PLS components. \n\nLet's update the recipe: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\npls_rec <- \n  cell_rec %>% \n  step_pls(all_predictors(), outcome = vars(class), num_comp = tune())\n\npls_wflow <- workflow(pls_rec, logistic_reg())\n```\n:::\n\n\n## Tuning PLS preprocessing\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npls_grid <- tibble(num_comp = 1:50)\n\npls_res <-\n  pls_wflow %>%\n  tune_grid(\n    resamples = cell_rs,\n    grid = pls_grid,\n    control = ctrl,\n    metrics = cls_metrics\n  )\n```\n:::\n\n\n## Tuning PLS preprocessing\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(pls_res, metric = \"brier_class\")\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/pls-plot-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Updating our recipe\n\nIf we are happy with this model, can can substitute a real value into the workflow (instead of `tune()`). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbest_pls <- select_best(pls_res, metric = \"brier_class\")\nbest_pls\n\npls_wflow <- \n  pls_wflow %>% \n  finalize_workflow(best_pls)\n#> # A tibble: 1 Ã— 2\n#>   num_comp .config              \n#>      <int> <chr>                \n#> 1       39 Preprocessor39_Model1\n```\n:::\n\n\n\n## The final fit   {.annotation}\n\nSuppose that we are happy with our PLS/logistic model.\n\nLet's fit the model on the training set and verify our performance using the test set.\n\n. . .\n\nWe've shown you `fit()` and `predict()` (+ `augment()`) but there is a shortcut:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# cell_split has train + test info\nfinal_fit <- last_fit(pls_wflow, cell_split, metrics = cls_metrics) \n\nfinal_fit\n#> # Resampling results\n#> # Manual resampling \n#> # A tibble: 1 Ã— 6\n#>   splits             id               .metrics .notes   .predictions .workflow \n#>   <list>             <chr>            <list>   <list>   <list>       <list>    \n#> 1 <split [1615/404]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n\n\n## Test set statistics\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(final_fit)\n#> # A tibble: 3 Ã— 4\n#>   .metric     .estimator .estimate .config             \n#>   <chr>       <chr>          <dbl> <chr>               \n#> 1 kap         binary         0.615 Preprocessor1_Model1\n#> 2 brier_class binary         0.123 Preprocessor1_Model1\n#> 3 roc_auc     binary         0.902 Preprocessor1_Model1\n```\n:::\n\n\n. . .\n\nThese are metrics computed with the **test** set\n\n## Test set predictions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_predictions(final_fit)\n#> # A tibble: 404 Ã— 7\n#>    id               .pred_PS .pred_WS  .row .pred_class class .config           \n#>    <chr>               <dbl>    <dbl> <int> <fct>       <fct> <chr>             \n#>  1 train/test split   0.994   0.00622     1 PS          PS    Preprocessor1_Modâ€¦\n#>  2 train/test split   0.0193  0.981       6 WS          WS    Preprocessor1_Modâ€¦\n#>  3 train/test split   0.0229  0.977       7 WS          WS    Preprocessor1_Modâ€¦\n#>  4 train/test split   0.617   0.383      10 PS          WS    Preprocessor1_Modâ€¦\n#>  5 train/test split   0.0146  0.985      11 WS          WS    Preprocessor1_Modâ€¦\n#>  6 train/test split   0.669   0.331      14 PS          PS    Preprocessor1_Modâ€¦\n#>  7 train/test split   0.0557  0.944      18 WS          WS    Preprocessor1_Modâ€¦\n#>  8 train/test split   0.995   0.00549    24 PS          PS    Preprocessor1_Modâ€¦\n#>  9 train/test split   0.979   0.0207     28 PS          PS    Preprocessor1_Modâ€¦\n#> 10 train/test split   0.981   0.0192     32 PS          WS    Preprocessor1_Modâ€¦\n#> # â„¹ 394 more rows\n```\n:::\n\n\n. . .\n\nThese are predictions for the **test** set\n\n## \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_predictions(final_fit) %>%\n  ggplot(aes(.pred_PS)) + \n    geom_histogram(col = \"white\", bins = 30) + \n    facet_wrap(~ class, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center'}\n:::\n:::\n\n\n## What is in `final_fit`? \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_workflow(final_fit)\n#> â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> 3 Recipe Steps\n#> \n#> â€¢ step_YeoJohnson()\n#> â€¢ step_normalize()\n#> â€¢ step_pls()\n#> \n#> â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#> \n#> Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#> (Intercept)        PLS01        PLS02        PLS03        PLS04        PLS05  \n#>   -1.151505     0.629591     0.272728     0.147008     0.142539     0.516826  \n#>       PLS06        PLS07        PLS08        PLS09        PLS10        PLS11  \n#>   -0.235048    -0.154358    -0.344322     0.329816     0.347405     0.446255  \n#>       PLS12        PLS13        PLS14        PLS15        PLS16        PLS17  \n#>   -0.543208     0.061578     0.029988    -0.121318    -0.155766     0.164601  \n#>       PLS18        PLS19        PLS20        PLS21        PLS22        PLS23  \n#>   -0.135477    -0.014923    -0.180127     0.154311     0.432299    -0.070286  \n#>       PLS24        PLS25        PLS26        PLS27        PLS28        PLS29  \n#>    0.443569    -0.086953     0.295322    -0.279665     0.443832     0.394397  \n#>       PLS30        PLS31        PLS32        PLS33        PLS34        PLS35  \n#>   -0.653353    -0.593635     0.695435    -0.467769    -0.284499     0.348176  \n#>       PLS36        PLS37        PLS38        PLS39  \n#>    0.000507    -0.141687     0.151696     1.134896  \n#> \n#> Degrees of Freedom: 1614 Total (i.e. Null);  1575 Residual\n#> Null Deviance:\t    2103 \n#> Residual Deviance: 1171 \tAIC: 1251\n```\n:::\n\n\n. . .\n\nUse this for **prediction** on new data, like for deploying\n\n\n\n\n\n## Next steps\n\n\n-   [Document the model](https://vetiver.rstudio.com/learn-more/model-card.html).\n\n. . .\n\n-   [Deploy the model](https://vetiver.rstudio.com/get-started/).\n\n. . .\n\n-   Create an [applicability domain model](https://applicable.tidymodels.org/) to help monitor our data over time.\n\n. . .\n\n-   Use [explainers](https://www.tmwr.org/explain.html) to characterize the model and the predictions.\n\n",
    "supporting": [
      "part_6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}