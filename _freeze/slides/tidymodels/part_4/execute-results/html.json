{
  "hash": "eb8cfb72ce998859b0703dbfda43a421",
  "result": {
    "markdown": "---\ntitle: \"tidymodels - Evaluating models\"\nauthor: \"Max Kuhn\"\nimage: \"images/featured.png\"\nformat:\n  revealjs:\n    slide-number: true\n    code-line-numbers: true\n    footer: <https://nbisweden.github.io/raukr-2023>\n    include-before-body: styles/header.html\n    include-after-body: styles/footer-annotations.html\n    theme: [default, styles/tidymodels.scss]\n    width: 1280\n    height: 720\nknitr:\n  opts_chunk:\n    echo: true\n    collapse: true\n    comment: \"#>\"\n    fig.align: \"center\"\n\nfig-format: svg\n---\n\n\n\n## Previously...\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(doParallel)\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\ncl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))\nregisterDoParallel(cl)\n\ndata(cells, package = \"modeldata\")\ncells$case <- NULL\n\nset.seed(123)\ncell_split <- initial_split(cells, prop = 0.8, strata = class)\ncell_tr <- training(cell_split)\ncell_te <- testing(cell_split)\n\ntree_spec <- decision_tree() %>%  set_mode(\"classification\")\n\ntree_wflow <- workflow(class ~ ., tree_spec)\n\ntree_fit <- tree_wflow %>% fit(data = cell_tr) \n```\n:::\n\n\n\n## Metrics for model performance \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(tree_fit, new_data = cell_te) %>%\n  metrics(class, estimate = .pred_class, .pred_PS)\n#> # A tibble: 4 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 accuracy    binary         0.807\n#> 2 kap         binary         0.583\n#> 3 mn_log_loss binary         0.522\n#> 4 roc_auc     binary         0.851\n```\n:::\n\n\n. . .\n\n-   `kap`: is Cohen's Kappa (maximize)\n-   `mn_log_loss`: \"log loss\" aka negaive binomial likelihood (minimize)\n-   `roc_auc`: area under the ROC curve (maximize)\n\n## Metrics for model performance - hard predictions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(tree_fit, new_data = cell_te) %>%\n  # You should name the 'estimate' column:\n  accuracy(class, estimate = .pred_class)\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.807\n```\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(tree_fit, new_data = cell_te) %>%\n  conf_mat(class, estimate = .pred_class)\n#>           Truth\n#> Prediction  PS  WS\n#>         PS 218  36\n#>         WS  42 108\n```\n:::\n\n\nA nice `autoplot()` method exists for `conf_mat()`. \n\n## Metrics for model performance - soft predictions\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(tree_fit, new_data = cell_te) %>%\n  # no 'estimate' argument\n  roc_auc(class, .pred_PS)\n#> # A tibble: 1 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 roc_auc binary         0.851\n```\n:::\n\n\n<br>\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(tree_fit, new_data = cell_te) %>%\n  roc_curve(class, .pred_PS) %>% \n  slice(1:5)\n#> # A tibble: 5 × 3\n#>   .threshold specificity sensitivity\n#>        <dbl>       <dbl>       <dbl>\n#> 1   -Inf          0            1    \n#> 2      0.156      0            1    \n#> 3      0.221      0.0833       0.988\n#> 4      0.270      0.646        0.862\n#> 5      0.657      0.75         0.838\n```\n:::\n\n\nAlso an `autoplot()` method for ROC curves \n\n## Make your own combination\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncls_metrics <- metric_set(brier_class, roc_auc, kap)\n\naugment(tree_fit, new_data = cell_te) %>%\n  cls_metrics(class, estimate = .pred_class, .pred_PS)\n#> # A tibble: 3 × 3\n#>   .metric     .estimator .estimate\n#>   <chr>       <chr>          <dbl>\n#> 1 kap         binary         0.583\n#> 2 brier_class binary         0.143\n#> 3 roc_auc     binary         0.851\n```\n:::\n\n\nThere are _a lot_ of performance measures for each mode...\n\n##  {background-iframe=\"https://yardstick.tidymodels.org/reference/index.html\"}\n\n::: footer\n:::\n\n\n# ⚠️ DANGERS OF OVERFITTING ⚠️\n\n## Dangers of overfitting ⚠️\n\n![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-train-1.svg)\n\n## Dangers of overfitting ⚠️\n\n![](https://raw.githubusercontent.com/topepo/2022-nyr-workshop/main/images/tuning-overfitting-test-1.svg)\n\n\n\n# The testing data are precious 💎\n\n# How can we use the *training* data to compare and evaluate different models? 🤔\n\n##  {background-color=\"white\" background-image=\"https://www.tmwr.org/premade/resampling.svg\" background-size=\"80%\"}\n\n## Cross-validation\n\n![](https://www.tmwr.org/premade/three-CV.svg)\n\n## Cross-validation\n\n![](https://www.tmwr.org/premade/three-CV-iter.svg)\n\n\n## Cross-validation cell_tr\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvfold_cv(cell_tr) # v = 10 is default\n#> #  10-fold cross-validation \n#> # A tibble: 10 × 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [1453/162]> Fold01\n#>  2 <split [1453/162]> Fold02\n#>  3 <split [1453/162]> Fold03\n#>  4 <split [1453/162]> Fold04\n#>  5 <split [1453/162]> Fold05\n#>  6 <split [1454/161]> Fold06\n#>  7 <split [1454/161]> Fold07\n#>  8 <split [1454/161]> Fold08\n#>  9 <split [1454/161]> Fold09\n#> 10 <split [1454/161]> Fold10\n```\n:::\n\n\n## Cross-validation cell_tr\n\nWhat is in this?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncell_rs <- vfold_cv(cell_tr)\ncell_rs$splits[1:3]\n#> [[1]]\n#> <Analysis/Assess/Total>\n#> <1453/162/1615>\n#> \n#> [[2]]\n#> <Analysis/Assess/Total>\n#> <1453/162/1615>\n#> \n#> [[3]]\n#> <Analysis/Assess/Total>\n#> <1453/162/1615>\n```\n:::\n\n\n::: notes\nTalk about a list column, storing non-atomic types in dataframe\n:::\n\n## Cross-validation cell_tr\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvfold_cv(cell_tr, v = 5)\n#> #  5-fold cross-validation \n#> # A tibble: 5 × 2\n#>   splits             id   \n#>   <list>             <chr>\n#> 1 <split [1292/323]> Fold1\n#> 2 <split [1292/323]> Fold2\n#> 3 <split [1292/323]> Fold3\n#> 4 <split [1292/323]> Fold4\n#> 5 <split [1292/323]> Fold5\n```\n:::\n\n\n## Cross-validation cell_tr\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvfold_cv(cell_tr, strata = class)\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 × 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [1453/162]> Fold01\n#>  2 <split [1453/162]> Fold02\n#>  3 <split [1453/162]> Fold03\n#>  4 <split [1453/162]> Fold04\n#>  5 <split [1453/162]> Fold05\n#>  6 <split [1454/161]> Fold06\n#>  7 <split [1454/161]> Fold07\n#>  8 <split [1454/161]> Fold08\n#>  9 <split [1454/161]> Fold09\n#> 10 <split [1454/161]> Fold10\n```\n:::\n\n\n. . .\n\nStratification often helps, with very little downside\n\n## Cross-validation cell_tr\n\nWe'll use this setup:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\ncell_rs <- vfold_cv(cell_tr, v = 10, strata = class)\ncell_rs\n#> #  10-fold cross-validation using stratification \n#> # A tibble: 10 × 2\n#>    splits             id    \n#>    <list>             <chr> \n#>  1 <split [1453/162]> Fold01\n#>  2 <split [1453/162]> Fold02\n#>  3 <split [1453/162]> Fold03\n#>  4 <split [1453/162]> Fold04\n#>  5 <split [1453/162]> Fold05\n#>  6 <split [1454/161]> Fold06\n#>  7 <split [1454/161]> Fold07\n#>  8 <split [1454/161]> Fold08\n#>  9 <split [1454/161]> Fold09\n#> 10 <split [1454/161]> Fold10\n```\n:::\n\n\n. . .\n\nSet the seed when creating resamples\n\n# We are equipped with metrics and resamples!\n\n## Fit our model to the resamples\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_res <- fit_resamples(tree_wflow, cell_rs, metrics = cls_metrics)\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 4\n#>    splits             id     .metrics         .notes          \n#>    <list>             <chr>  <list>           <list>          \n#>  1 <split [1453/162]> Fold01 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  2 <split [1453/162]> Fold02 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  3 <split [1453/162]> Fold03 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  4 <split [1453/162]> Fold04 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  5 <split [1453/162]> Fold05 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  6 <split [1454/161]> Fold06 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  7 <split [1454/161]> Fold07 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  8 <split [1454/161]> Fold08 <tibble [3 × 4]> <tibble [0 × 3]>\n#>  9 <split [1454/161]> Fold09 <tibble [3 × 4]> <tibble [0 × 3]>\n#> 10 <split [1454/161]> Fold10 <tibble [3 × 4]> <tibble [0 × 3]>\n```\n:::\n\n\n## Evaluating model performance \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_res %>% collect_metrics()\n#> # A tibble: 3 × 6\n#>   .metric     .estimator  mean     n std_err .config             \n#>   <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 brier_class binary     0.137    10 0.00366 Preprocessor1_Model1\n#> 2 kap         binary     0.613    10 0.0144  Preprocessor1_Model1\n#> 3 roc_auc     binary     0.854    10 0.00563 Preprocessor1_Model1\n```\n:::\n\n\n. . .\n\nWe can reliably measure performance using only the **training** data 🎉\n\n\n## Evaluating model performance \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Save the assessment set results\nctrl_rs <- control_resamples(save_pred = TRUE)\ntree_res <- fit_resamples(tree_wflow, cell_rs, metrics = cls_metrics, control = ctrl_rs)\n\ntree_preds <- collect_predictions(tree_res)\ntree_preds\n#> # A tibble: 1,615 × 7\n#>    id     .pred_PS .pred_WS  .row .pred_class class .config             \n#>    <chr>     <dbl>    <dbl> <int> <fct>       <fct> <chr>               \n#>  1 Fold01    0.943   0.0566    10 PS          PS    Preprocessor1_Model1\n#>  2 Fold01    0.943   0.0566    20 PS          PS    Preprocessor1_Model1\n#>  3 Fold01    0.763   0.237     25 PS          PS    Preprocessor1_Model1\n#>  4 Fold01    0.214   0.786     31 WS          PS    Preprocessor1_Model1\n#>  5 Fold01    0.763   0.237     41 PS          PS    Preprocessor1_Model1\n#>  6 Fold01    0.943   0.0566    48 PS          PS    Preprocessor1_Model1\n#>  7 Fold01    0.943   0.0566    52 PS          PS    Preprocessor1_Model1\n#>  8 Fold01    0.214   0.786     60 WS          PS    Preprocessor1_Model1\n#>  9 Fold01    0.943   0.0566    61 PS          PS    Preprocessor1_Model1\n#> 10 Fold01    0.214   0.786     91 WS          PS    Preprocessor1_Model1\n#> # ℹ 1,605 more rows\n```\n:::\n\n\n## \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_preds %>% \n  ggplot(aes(.pred_PS)) + \n  geom_histogram(col = \"white\", bins = 30) + \n  facet_wrap(~ class, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](part_4_files/figure-revealjs/unnamed-chunk-16-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Where are the fitted models?   {.annotation}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntree_res\n#> # Resampling results\n#> # 10-fold cross-validation using stratification \n#> # A tibble: 10 × 5\n#>    splits             id     .metrics         .notes           .predictions\n#>    <list>             <chr>  <list>           <list>           <list>      \n#>  1 <split [1453/162]> Fold01 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  2 <split [1453/162]> Fold02 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  3 <split [1453/162]> Fold03 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  4 <split [1453/162]> Fold04 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  5 <split [1453/162]> Fold05 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  6 <split [1454/161]> Fold06 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  7 <split [1454/161]> Fold07 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  8 <split [1454/161]> Fold08 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#>  9 <split [1454/161]> Fold09 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n#> 10 <split [1454/161]> Fold10 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>\n```\n:::\n\n\n. . .\n\n🗑️\n\n# Alternate resampling schemes\n\n## Bootstrapping\n\n![](https://www.tmwr.org/premade/bootstraps.svg)\n\n## Bootstrapping cell_tr\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(3214)\nbootstraps(cell_tr)\n#> # Bootstrap sampling \n#> # A tibble: 25 × 2\n#>    splits             id         \n#>    <list>             <chr>      \n#>  1 <split [1615/603]> Bootstrap01\n#>  2 <split [1615/589]> Bootstrap02\n#>  3 <split [1615/574]> Bootstrap03\n#>  4 <split [1615/590]> Bootstrap04\n#>  5 <split [1615/591]> Bootstrap05\n#>  6 <split [1615/609]> Bootstrap06\n#>  7 <split [1615/599]> Bootstrap07\n#>  8 <split [1615/579]> Bootstrap08\n#>  9 <split [1615/605]> Bootstrap09\n#> 10 <split [1615/586]> Bootstrap10\n#> # ℹ 15 more rows\n```\n:::\n\n\n##  {background-iframe=\"https://rsample.tidymodels.org/reference/index.html\"}\n\n::: footer\n:::\n\n## Validation sets\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(853)\nvalidation_split(cell_tr, strata = class)\n#> # Validation Set Split (0.75/0.25)  using stratification \n#> # A tibble: 1 × 2\n#>   splits             id        \n#>   <list>             <chr>     \n#> 1 <split [1211/404]> validation\n```\n:::\n\n\n. . .\n\nA validation set is just another type of resample.\n\n. . . \n\nThis function will not go away but we have a better interface for validation \nin the next rsample release. \n\n# Decision tree 🌳\n\n# Random forest 🌳🌲🌴🌵🌴🌳🌳🌴🌲🌵🌴🌲🌳🌴🌳🌵🌵🌴🌲🌲🌳🌴🌳🌴🌲🌴🌵🌴🌲🌴🌵🌲🌵🌴🌲🌳🌴🌵🌳🌴🌳\n\n## Random forest 🌳🌲🌴🌵🌳🌳🌴🌲🌵🌴🌳🌵\n\n- Ensemble many decision tree models\n\n- All the trees vote! 🗳️\n\n- Bootstrap aggregating + random predictor sampling\n\n. . .\n\n- Often works well without tuning hyperparameters (more on this), as long as there are enough trees\n\n## Create a random forest model \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger\n```\n:::\n\n\n## Create a random forest model \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrf_wflow <- workflow(class ~ ., rf_spec)\nrf_wflow\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Formula\n#> Model: rand_forest()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> class ~ .\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Random Forest Model Specification (classification)\n#> \n#> Main Arguments:\n#>   trees = 1000\n#> \n#> Computational engine: ranger\n```\n:::\n\n\n\n## Evaluating model performance \n\n\n::: {.cell layout-align=\"center\" hash='part_4_cache/revealjs/unnamed-chunk-22_eafc24814963d80fd885aa54b9b92f28'}\n\n```{.r .cell-code}\nctrl_rs <- control_resamples(save_pred = TRUE)\n\n# Random forest uses random numbers so set the seed first\n\nset.seed(2)\nrf_res <- fit_resamples(rf_wflow, cell_rs, control = ctrl_rs, metrics = cls_metrics)\ncollect_metrics(rf_res)\n#> # A tibble: 3 × 6\n#>   .metric     .estimator  mean     n std_err .config             \n#>   <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n#> 1 brier_class binary     0.120    10 0.00283 Preprocessor1_Model1\n#> 2 kap         binary     0.625    10 0.0163  Preprocessor1_Model1\n#> 3 roc_auc     binary     0.903    10 0.00495 Preprocessor1_Model1\n```\n:::\n\n\n## \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_predictions(rf_res) %>% \n  ggplot(aes(.pred_PS)) + \n    geom_histogram(col = \"white\", bins = 30) + \n    facet_wrap(~ class, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](part_4_files/figure-revealjs/unnamed-chunk-23-1.svg){fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}