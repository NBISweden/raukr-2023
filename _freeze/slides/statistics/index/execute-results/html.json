{
  "hash": "fa24ab238e9b04f6fd2400561d8d63e1",
  "result": {
    "markdown": "---\ntitle: \"Mathematical Statistics in R\"\nauthor: \"Nikolay Oskolkov\"\nimage: \"assets/featured.jpg\"\nformat: revealjs\n---\n\n\n## Packages {visibility=\"hidden\"}\n\n\n::: {.cell}\n\n:::\n\n\n## What is Mathematical Statistics?\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- Can Mathematical Statistics mean\n  - a statistical test? \n  - a probability distribution? \n  - or maybe a p-value?\n\n- We are not going to draw marbles or roll dice\n\n:::\n::: {.column width=\"50%\"}\n\n![](assets/Dice.jpg)\n\n:::\n::::\n\n[**Classic statistics is not the only way to analyze your data**]{style=\"color:red;\" .center}\n\n## Different Types of Data Analysis\n\n- Depends on the amount of data we have\n- Balance between the numbers of features and observations\n  - P is the number of features (genes, proteins, genetic variants etc.)\n  - N is the number of observations (samples, cells, nucleotides etc.)\n\n![](assets/AmountOfData.png){width=\"90%\"}\n\n## Do we have Big Data in Life Sciences?\n\n![](assets/BigData.png)\n\n## Precision Medicine: Why isn't it in the Clinics?\n\n![](assets/PrecisionMedicine.png)\n\n## The Curse of Dimensionality\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n![](assets/DarkMagic.jpg){width=\"60%\"}\n\n:::\n::: {.column width=\"60%\"}\n\n$$Y = \\alpha + \\beta X$$\n$$\\beta = \\left(X^TX\\right)^{-1}X^TY$$\n\n$$\\left(X^TX\\right)^{-1} \\sim \\frac{1}{\\rm{det}\\left(X^TX\\right)}\\dots\\,\\rightarrow\\,\\infty\\hbox{,}\\,\\,\\,\\,\\,\\,\\,\\,n\\ll p$$\n\n<br/>\n\n[**The math blows up in high dimensions**]{style=\"color:red;\" .center}\n\n<br/>\n\n![](assets/ndim_ball.png)\n\n<br/>\n$$l_d = r\\sqrt{d}\\hbox{, where d is dimensionality of space}$$\n\n<br/>\n\n[**Euclidean distance fails, hard to define metric**]{style=\"color:red;\" .center}\n\n:::\n::::\n\n## Low Dimensional Space\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 20 # number of samples\np <- 2 # number of features / dimensions\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0522 -0.6380  0.1451  0.3911  1.8829 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.14950    0.22949   0.651    0.523\nX1          -0.09405    0.28245  -0.333    0.743\nX2          -0.11919    0.24486  -0.487    0.633\n\nResidual standard error: 1.017 on 17 degrees of freedom\nMultiple R-squared:  0.02204,\tAdjusted R-squared:  -0.09301 \nF-statistic: 0.1916 on 2 and 17 DF,  p-value: 0.8274\n```\n:::\n:::\n\n\n## Going to Higher Dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nn <- 20\np <- 10\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0255 -0.4320  0.1056  0.4493  1.0617 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.54916    0.26472   2.075   0.0679 .\nX1           0.30013    0.21690   1.384   0.1998  \nX2           0.68053    0.27693   2.457   0.0363 *\nX3          -0.10675    0.26010  -0.410   0.6911  \nX4          -0.21367    0.33690  -0.634   0.5417  \nX5          -0.19123    0.31881  -0.600   0.5634  \nX6           0.81074    0.25221   3.214   0.0106 *\nX7           0.09634    0.24143   0.399   0.6992  \nX8          -0.29864    0.19004  -1.571   0.1505  \nX9          -0.78175    0.35408  -2.208   0.0546 .\nX10          0.83736    0.36936   2.267   0.0496 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8692 on 9 degrees of freedom\nMultiple R-squared:  0.6592,\tAdjusted R-squared:  0.2805 \nF-statistic: 1.741 on 10 and 9 DF,  p-value: 0.2089\n```\n:::\n:::\n\n\n## Even Higher Dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123456)\nn <- 20\np <- 20\nY <- rnorm(n)\nX <- matrix(rnorm(n * p), n, p)\nsummary(lm(Y ~ X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\nALL 20 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  1.34889        NaN     NaN      NaN\nX1           0.66218        NaN     NaN      NaN\nX2           0.76212        NaN     NaN      NaN\nX3          -1.35033        NaN     NaN      NaN\nX4          -0.57487        NaN     NaN      NaN\nX5           0.02142        NaN     NaN      NaN\nX6           0.40290        NaN     NaN      NaN\nX7           0.03313        NaN     NaN      NaN\nX8          -0.31983        NaN     NaN      NaN\nX9          -0.92833        NaN     NaN      NaN\nX10          0.18091        NaN     NaN      NaN\nX11         -1.37618        NaN     NaN      NaN\nX12          2.11438        NaN     NaN      NaN\nX13         -1.75103        NaN     NaN      NaN\nX14         -1.55073        NaN     NaN      NaN\nX15          0.01112        NaN     NaN      NaN\nX16         -0.50943        NaN     NaN      NaN\nX17         -0.47576        NaN     NaN      NaN\nX18          0.31793        NaN     NaN      NaN\nX19          1.43615        NaN     NaN      NaN\nX20               NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1,\tAdjusted R-squared:    NaN \nF-statistic:   NaN on 19 and 0 DF,  p-value: NA\n```\n:::\n:::\n\n\n## Dimensionality Reduction\n\n\n::: {.cell hash='index_cache/revealjs/unnamed-chunk-5_52eac487ae0d539f24b9b0bd27c4f259'}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n<br/>\n\n[**Dimensionality Reduction is not for visualization but overcoming the Curse of Dimensionality**]{style=\"color:red;\" .center}\n\n## Linear Dimensionality Reduction\n\n![](assets/LinearDimReduct.png){width=\"90%\"}\n\n## Non-Linear Dimensionality Reduction\n\n![](assets/NonLinearDimReduct.png){width=\"90%\"}\n\n## Frequentist Statistics Failure\n\n![](assets/Anscombes_quartet.png){width=\"90%\"}\n\n## Frequentist Statistics is Brain Damaging\n\n![](assets/DataSaurus.gif){width=\"55%\"}\n\n![](assets/BoxViolinSmaller.gif){width=\"55%\"}\n\n## Pvalue is not good for ranking features\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](assets/Pvalue.png){width=\"100%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFC <- 1.02\nx_mean <- 5\nx_sd <- 1\nN_vector <- seq(from = 100, to = 10000, by = 100)\nx1 <- rnorm(N_vector, x_mean, x_sd)\nx2 <- rnorm(N_vector, x_mean * FC, x_sd)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n::::\n\n## Maximum Likelihood Principle\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n- We maximize probability to observe the data $X_i$\n\n$$\\rm{L}\\,(\\,\\rm{X_i} \\,|\\, \\mu,\\sigma\\,) =\n\\prod_{i=0}^{N}\\frac{1}{\\sqrt{2\\pi\\sigma²}} \\exp^{\\displaystyle -\\frac{(X_i-\\mu)^2}{2\\sigma²}}\\\\\n\\mu = \\frac{1}{N}\\sum_{i=0}^N \\rm{X_i}\\\\\n\\sigma^2 = \\frac{1}{N}\\sum_{i=0}^N (\\rm{X_i}-\\mu)^2$$\n\n:::\n\n::: {.column width=\"40%\"}\n\n- Maximum Likelihood has many assumptions:\n  - Large sample size\n  - Gaussian distribution\n  - Homoscedasticity\n  - Uncorrelated errors\n  - Convergence of covariance\n- Those assumptions are not fulfilled in the real world\n\n:::\n::::\n\n## Two-Groups Statistical Test\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(12)\nX <- c(rnorm(20, mean = 5, sd = 2), 12, 15, 14, 16)\nY <- c(rnorm(24, mean = 7, sd = 2))\nboxplot(X, Y, ylab = \"DIFFERENCE\", names = c(\"X\", \"Y\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Parametric Statistical Test Fails\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(X, Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  X and Y\nt = -1.084, df = 31.678, p-value = 0.2865\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.8819545  0.8804344\nsample estimates:\nmean of x mean of y \n 5.989652  6.990412 \n```\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n:::\n::::\n\n## Resampling\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nobserved <- median(Y) - median(X)\nprint(paste0(\"Observed difference = \", observed))\nres <- vector(length = 1000)\nfor (i in 1:1000) {\n    Z <- c(X, Y)\n    Y_res <- sample(Z, length(Y), FALSE)\n    X_res <- sample(Z, length(X), FALSE)\n    res[i] <- median(Y_res) - median(X_res)\n}\nhist(abs(res), breaks = 100, main = \"Resampled\", xlab = \"Difference\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=768}\n:::\n\n```{.r .cell-code}\nprint(paste0(\"p-value = \", sum(abs(res) >= abs(observed)) / 1000))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Observed difference = 2.33059085402647\"\n[1] \"p-value = 0.001\"\n```\n:::\n:::\n\n\n## ML Does Not Stand Non-Independence\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n           n1         n2          n3           n4         n5\np1 -0.6760258 -1.2307634  1.66039982  0.196033326 -0.2981471\np2 -1.5834993  0.6494188 -0.01267663 -1.064763128 -0.1792141\np3  0.3152418 -0.5791937 -1.79593465 -0.312303710  0.2671534\np4 -0.9359010  0.1212546 -0.36279328 -0.553364109  1.0598898\np5 -2.0411903  0.6899356 -1.03923098  0.008958754 -0.2249498\n```\n:::\n:::\n\n\n- Two types of non-independence in data\n  - between samples\n  - between features\n\n## ML Does Not Stand Non-Independence\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n### Random Effects\n\n![](assets/Random_Effects.jpg){width=\"60%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n### Lasso\n\n![](assets/lasso.jpg){width=\"40%\"}\n\n:::\n::::\n\n## Linear Model with Non-Independence\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"lme4\")\nlibrary(\"ggplot2\")\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Fit Linear Model for Each Individual\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n    geom_smooth(method = \"lm\", level = 0.95) +\n    geom_point() +\n    facet_wrap(~Subject, nrow = 3, ncol = 6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Random Effects and Missing Heritability\n\n![](assets/MissingHeritability.png){width=\"80%\" .center}\n\n## Random Effects Modelling\n\n- Allow individual level Slopes and Intercepts\n- This is nothing else than Bayesian Priors on coefficients\n\n$$\\rm{Reaction} = \\alpha_i + \\beta_i \\rm{Days}$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmerfit <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n```\n:::\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n[**Shrinkage: Introduce shared variance parameter**]{style=\"color:red;\" .center}\n\n## Linear Mixed Models (LMM)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lmer(Reaction ~ Days + (Days | Subject), sleepstudy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n```\n:::\n:::\n\n\n## LMM Average Fit\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/revealjs/unnamed-chunk-20_750ad8bbf5288a21f52f537a3d227de1'}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## LMM Individual Fit\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/revealjs/unnamed-chunk-21_3b474c66f6ef3a0a696a3be4c4c7864f'}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## What is Bayesian Statistics for You?\n\n![](assets/BayesianStatistics.png){width=\"90%\"}\n\n::: {.small}\n\n* **Handling Missing Data**\n* **Handling Non-Gaussian Data**\n* **Cause or Consequence**\n* **Lack of Statistical Power**\n* **Overfitting and Correction for Multiple Testing (FDR)**\n* **Testing for Significance and P-Value**\n\n:::\n\n## Frequentist vs. Bayesian Linear Model\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n### Maximum Likelihood\n\n$$y = \\alpha+\\beta x$$\n\n<br/>\n\n$$L(y) \\sim e^{-\\frac{(y-\\alpha-\\beta x)^2}{2\\sigma^2}}$$\n\n<br/>\n\n$$\\max_{\\alpha,\\beta,\\sigma}L(y) \\Longrightarrow \\hat\\alpha, \\hat\\beta, \\hat\\sigma$$\n\n:::\n::: {.column width=\"50%\"}\n\n### Bayesian Linear Fitting\n\n$$y \\sim \\it N(\\mu,\\sigma) \\quad\\textrm{- Likelihood L(y)}$$\n\n<br/>\n\n$$\\mu = \\alpha + \\beta x$$\n\n<br/>\n\n$$\\alpha \\sim \\it N(\\mu_\\alpha,\\sigma_\\alpha) \\quad\\textrm{- Prior on} \\quad\\alpha \\\\\n\\beta \\sim \\it N(\\mu_\\beta,\\sigma_\\beta) \\quad\\textrm{- Prior on} \\quad\\beta$$\n\n<br/>\n\n$$P(\\mu_\\alpha,\\sigma_\\alpha,\\mu_\\beta,\\sigma_\\beta,\\sigma) \\sim  L(y)N(\\mu_\\alpha,\\sigma_\\alpha)N(\\mu_\\beta,\\sigma_\\beta)$$\n\n<br/>\n\n$$\\max_{\\mu_\\alpha,\\sigma_\\alpha,\\mu_\\beta,\\sigma_\\beta,\\sigma}P(\\mu_\\alpha,\\sigma_\\alpha,\\mu_\\beta,\\sigma_\\beta,\\sigma) \\Longrightarrow \\hat\\mu_\\alpha,\\hat\\sigma_\\alpha,\\hat\\mu_\\beta,\\hat\\sigma_\\beta,\\hat\\sigma$$\n\n:::\n::::\n\n## Bayesian Linear Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"brms\")\noptions(mc.cores = parallel::detectCores())\nbrmfit <- brm(Reaction ~ Days + (Days | Subject), data = sleepstudy)\nsummary(brmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Reaction ~ Days + (Days | Subject) \n   Data: sleepstudy (Number of observations: 180) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~Subject (Number of levels: 18) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)          26.60      6.84    15.10    42.23 1.00     1788     2248\nsd(Days)                6.51      1.50     4.15    10.04 1.00     1355     1922\ncor(Intercept,Days)     0.08      0.31    -0.50     0.68 1.00      878     1282\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   251.04      7.23   236.60   265.43 1.00     1663     2405\nDays         10.52      1.72     7.23    14.01 1.00     1528     1826\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    25.92      1.61    23.01    29.32 1.00     4021     2491\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n## Bayesian Population-Level Fit\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-23-1.png){width=768}\n:::\n:::\n\n\n## Bayesian Group-Level Fit\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-24-1.png){width=768}\n:::\n:::\n\n\n## Feature Non-Independence: LASSO\n\n$$Y = \\beta_1X_1+\\beta_2X_2+\\epsilon \\\\\n\\textrm{OLS} = (y-\\beta_1X_1-\\beta_2X_2)^2 \\\\\n\\textrm{Penalized OLS} = (y-\\beta_1X_1-\\beta_2X_2)^2 + \\lambda(|\\beta_1|+|\\beta_2|)$$\n\n![](assets/CV_lambda.png){width=\"70%\" .center}\n\n## Causality Inference: get rid of confounders\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n$$Y \\sim X + C\\hbox{,  where C is a confounder}$$\n\n![](assets/IV.png){width=\"70%\" .center}\n\n:::\n::: {.column width=\"50%\"}\n\n[**Mendelian Randomization**]{style=\"color:red;\" .center}\n\n![](assets/InsEndCancer.png){width=\"70%\" .center}\n\n[**Method of Instrumental Variables**]{style=\"color:red;\" .center}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1e3 # Number of Observations\nC <- rnorm(n = N)\nX <- 2 * C + rnorm(n = N)\nY <- 2 * X + 2 * C + rnorm(n = N)\nsummary(lm(Y ~ X))\n```\n:::\n\n\n:::\n::::\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4905 -0.8991  0.0072  0.9051  4.8766 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.04544    0.04245    1.07    0.285    \nX            2.79987    0.01876  149.27   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.339 on 998 degrees of freedom\nMultiple R-squared:  0.9571,\tAdjusted R-squared:  0.9571 \nF-statistic: 2.228e+04 on 1 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Known confounders: Controlled Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Y ~ X + C))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X + C)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1141 -0.6955 -0.0089  0.7270  3.8172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06693    0.03250    2.06   0.0397 *  \nX            2.00552    0.03314   60.52   <2e-16 ***\nC            1.97305    0.07419   26.59   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.025 on 997 degrees of freedom\nMultiple R-squared:  0.9749,\tAdjusted R-squared:  0.9749 \nF-statistic: 1.938e+04 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n<br/>\n\n[**What if confounders are not known?**]{style=\"color:red;\" .center}\n\n## Good Instrumental Variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nlibrary(\"AER\")\nN <- 1e3 # Number of Observations\nweak <- rnorm(n = N)\ngood <- rnorm(n = N)\nC <- rnorm(n = N)\nX <- 0.000001 * weak + 2 * good + 2 * C + rnorm(n = N)\nY <- 2 * X + 2 * C + rnorm(n = N)\ndf <- cbind(Y, X, C, good, weak)\ncolnames(df) <- c(\"Y\", \"X\", \"C\", \"good\", \"weak\")\ndf <- data.frame(df)\nsummary(ivreg(Y ~ X | good, data = df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nivreg(formula = Y ~ X | good, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-7.78427 -1.48164  0.05357  1.37862  6.99009 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.01017    0.06867   0.148    0.882    \nX            2.02609    0.03207  63.183   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.172 on 998 degrees of freedom\nMultiple R-Squared: 0.9237,\tAdjusted R-squared: 0.9236 \nWald test:  3992 on 1 and 998 DF,  p-value: < 2.2e-16 \n```\n:::\n:::\n\n\n## Weak Instrumental Variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ivreg(Y ~ X | weak, data = df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nivreg(formula = Y ~ X | weak, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.319 -1.157  0.027  1.218  5.711 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.003767   0.056301   0.067    0.947    \nX           2.458141   0.401503   6.122 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.77 on 998 degrees of freedom\nMultiple R-Squared: 0.9493,\tAdjusted R-squared: 0.9492 \nWald test: 37.48 on 1 and 998 DF,  p-value: 1.324e-09 \n```\n:::\n:::\n\n\n## What does ivreg do under the hood?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Y ~ as.numeric(predict(lm(X ~ good)))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ as.numeric(predict(lm(X ~ good))))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.8345  -4.2676  -0.0506   4.3873  20.1810 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                        0.01017    0.20723   0.049    0.961    \nas.numeric(predict(lm(X ~ good)))  2.02609    0.09677  20.938   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.553 on 998 degrees of freedom\nMultiple R-squared:  0.3052,\tAdjusted R-squared:  0.3045 \nF-statistic: 438.4 on 1 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Lasso helps to build good Instrumental Variables\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](assets/CausalXonY.png)\n\n:::\n::: {.column width=\"50%\"}\n\n![](assets/LassoIV.png)\n\n:::\n::::\n\n## Simplex Space: a few words about normalization\n\nWe draw two vectors (genes) from normal distributions with different means and sds\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ngene1 <- rnorm(100, mean = 10, sd = 1)\ngene2 <- rnorm(100, mean = 100, sd = 5)\ncor.test(gene1, gene2, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  gene1 and gene2\nS = 166000, p-value = 0.9693\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.00390039 \n```\n:::\n:::\n\n\n## Library Size Normalization\n\nWe create an expression data frame with 100 samples and 2 genes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpr <- as.data.frame(t(data.frame(gene1 = gene1, gene2 = gene2)))\ncolnames(expr) <- paste0(\"sample\", seq(1:100))\nexpr[1:2, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        sample1    sample2  sample3  sample4\ngene1  9.439524   9.769823 11.55871 10.07051\ngene2 96.447967 101.284419 98.76654 98.26229\n```\n:::\n:::\n\n\nWe calculate library size normalized expression data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpr_n <- apply(expr, 2, function(x) x / sum(x))\nexpr_n[1:2, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         sample1    sample2   sample3  sample4\ngene1 0.08914674 0.08797343 0.1047694 0.092959\ngene2 0.91085326 0.91202657 0.8952306 0.907041\n```\n:::\n:::\n\n\n## Spurious Correlation After Normalization\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n[**Before Library Size Normalization**]{style=\"color:red;\" .center}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(expr1, expr2, xlab = \"gene1\", ylab = \"gene2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-35-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n[**After Library Size Normalization**]{style=\"color:red;\" .center}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(expr_n1, expr_n2, xlab = \"gene1\", ylab = \"gene2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n::::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(expr_n1, expr_n2, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  expr_n1 and expr_n2\nS = 333300, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\nrho \n -1 \n```\n:::\n:::\n\n\n## OMICs Integration\n\n![](assets/OMICsIntegr.png){fig-align=\"center\"}\n\n## Single Cell OMICs Integration\n\n![](assets/SingleCellMultiOmics.png){fig-align=\"center\"}\n\n## Single Cell OMICs Integration\n\n![](assets/scRNAseqIntegr.png){fig-align=\"center\"}\n\n## scNMT OMICs Integration\n\n![](assets/scNMT.png){fig-align=\"center\"}\n\n## {background-image=\"../../assets/images/cover.jpg\"}\n\n### Thank you! Questions?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n         _                     \nplatform x86_64-conda-linux-gnu\nos       linux-gnu             \nmajor    4                     \nminor    2.2                   \n```\n:::\n:::\n\n\n[2023 • [SciLifeLab](https://www.scilifelab.se/) • [NBIS](https://nbis.se/) • [RaukR](https://nbisweden.github.io/workshop-RaukR-2306/)]{.smaller}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}